{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799ed361-7d31-4b8a-9593-624ed1f6d0f0",
   "metadata": {},
   "source": [
    "# ðŸ“° News Recommendation System Part 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78310ec5-35b2-4a7d-a40e-bb3dc59f554e",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Project Introduction\n",
    "This project explores user behavior prediction in a news recommendation scenario. The goal is to build a model that can predict a user's future click behavior based on their historical browsing and clicking behavior data, specifically the last news article they clicked on.\n",
    "\n",
    "The setting is inspired by a real-world news app, where delivering timely, relevant content is essential for user engagement. This project aims to simulate a practical recommender system, combining business intuition with machine learning techniques to address a realistic problem in the content recommendation space.\n",
    "\n",
    "## ðŸ“Š Data Overview\n",
    "The dataset contains user interaction data from a large-scale news platform, including:\n",
    "- 300,000 users\n",
    "- ~3 million clicks\n",
    "- 360,000+ unique news articles; each news article is represented by a pre-trained embedding vector, capturing semantic relationships between articles.\n",
    "\n",
    "We extracted click log data from 200,000 users as the training set, 50,000 users as test set A, and 50,000 users as test set B.\n",
    "\n",
    "## ðŸ“„ Data Tables\n",
    "\n",
    "- train_click_log.csv: Training set user click logs\n",
    "- testA_click_log.csv: Test set user click logs\n",
    "- articles.csv: News article information data table\n",
    "- articles_emb.csv: Embedding vector representation of news articles\n",
    "\n",
    "|        **Field**        |         **Description**          |\n",
    "| :---------------------: | :------------------------------: |\n",
    "|         user_id         |              User ID             |\n",
    "|    click_article_id     |            Clicked article ID    |\n",
    "|     click_timestamp     |            Click timestamp        |\n",
    "|    click_environment    |             Click environment     |\n",
    "|    click_deviceGroup    |            Click device group     |\n",
    "|        click_os         |           Click operating system  |\n",
    "|      click_country      |             Click city            |\n",
    "|      click_region       |             Click region          |\n",
    "|   click_referrer_type   |           Click source type       |\n",
    "|       article_id        | Article ID, corresponding to click_article_id |\n",
    "|       category_id       |            Article type ID        |\n",
    "|      created_at_ts      |          Article creation timestamp |\n",
    "|       words_count       |             Article word count     |\n",
    "| emb_1,emb_2,...,emb_249 |      Article embedding vector representation |\n",
    "\n",
    "## ðŸ“ Evaluation Metrics\n",
    "The final recommendation for each user will include five recommended articles, sorted by click probability.\n",
    "\n",
    "For example, for user1, our recommendation would be:\n",
    "> user1, article1, article2, article3, article4, article5.\n",
    "\n",
    "There is only one correct answer for each user's last clicked article, so we check if any of the recommended five articles match the actual answer. We will use **mean reciprocal rank** as the evaluation metric. The formula is as follows:\n",
    "$$\n",
    "score(user) = \\sum_{k=1}^5 \\frac{s(user, k)}{k}\n",
    "$$\n",
    "\n",
    "If article1 is the actual article clicked by the user, then s(user1, 1) = 1, and s(user1, 2-4) are all 0. If article2 is the article clicked by the user, then s(user, 2) = 1/2, and s(user, 1, 3, 4, 5) are all 0. Thus, score(user) = the reciprocal of the rank at which the match occurs. If there are no matches, score(user1) = 0. This is reasonable because we want hits to be as high-ranking as possible, which yields a higher score.\n",
    "\n",
    "## ðŸ’¡ Project Understanding\n",
    "The goal of this project is to **predict the last news article a user clicked, based on their historical browsing data**. Unlike traditional structured prediction problems, this is more aligned with real-world recommendation systems, using raw user click logs rather than neatly labeled data.\n",
    "\n",
    "To approach this, I framed the task as a **supervised learning** problem by transforming user-article interactions into \"features + labels\" training data. The core idea is to predict the likelihood of a user clicking a given article, turning this into a click-through rate (CTR) prediction task. This reframing allows for the use of **classification models**â€”starting with simple baselines like logistic regression and moving toward deep learning approaches.\n",
    "\n",
    "Now, we have converted this problem into a classification problem, where the classification label is whether the user will click on a particular article. The features of the classification problem will include the user and the article. We need to train a classification model to predict the probability of a particular user clicking on a specific article. This raises several additional questions:\n",
    "- How to create training and testing datasets?\n",
    "- What specific features can we leverage?\n",
    "- What models can we attempt?\n",
    "- With 360,000 articles and over 200,000 users, what strategies do we have to reduce the problem's scale? How do we make the final predictions?\n",
    "\n",
    "**For the third part, we will prepare new features for each user's recalled list. These feature will be used in the ranking model in the fourth part.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d556b-baae-4823-97ea-d94d7880dc88",
   "metadata": {},
   "source": [
    "## Converting to a Supervised Learning Problem\n",
    "To prepare our data for machine learning, we need to convert it into a supervised learning problem with features and labels. The features we can use directly from our initial data are:\n",
    "\n",
    "- **Article Features**: These are the inherent properties of each article, such as its category_id, created_at_ts (creation timestamp, which indicates its timeliness), and words_count. These help us understand user preferences for content type, newness, and length.\n",
    "\n",
    "- **Article Embedding Features**: These are the vector representations of article content, which we've used in the recall phase. We can also use Word2Vec or BERT to create new embedding features that contain semantic relationships.\n",
    "\n",
    "- **User Device Features**: Information about the user's device provides a useful context for their interactions.\n",
    "\n",
    "## Building the Supervised Dataset\n",
    "\n",
    "Our recall phase gives us a dictionary in the format {user_id: [list of potential articles]}. We can use this to build our training set. For each user and each potential article in their list, we'll create a data point. For example, if user1's recall list is {user1: [item1, item2, item3]}, our dataset will have three rows: (user1, item1), (user1, item2), and (user1, item3). These also form the first two feature columns of our supervised test set.\n",
    "\n",
    "## Constructing New Features from Historical Behavior\n",
    "\n",
    "A key insight from the data analysis (Part 1) is that a user's final click is strongly correlated with their recent clicks. Therefore, our most important features will be a combination of the user's historical behavior and the candidate article. For each candidate article, we will create the following features based on its relationship to the user's most recent clicks:\n",
    "\n",
    "1. Similarity Features: Calculate the similarity (e.g., using the inner product of embeddings) between the candidate item and the last few articles the user clicked. This directly captures the user's most recent interests.\n",
    "\n",
    "2. Statistical Features of Similarity: Compute statistical measures like the average or standard deviation of the similarity features. This can help smooth out noise and capture broader trends in a user's preferences.\n",
    "\n",
    "3. Word Count Difference: Calculate the difference in word count between the candidate item and the user's most recently clicked articles. This can reveal preferences for article length.\n",
    "\n",
    "4. Time Difference: Calculate the time gap between the candidate article's creation time and the user's last click time. This is a powerful feature for understanding a user's preference for timely content.\n",
    "\n",
    "5. User-item similarity obtained from YouTubeDNN recall: Create a similarity feature between the user and the candidate item itself, which can be very informative.\n",
    "\n",
    "In this part, we will implement the creation of these features. The logic is as follows:\n",
    "1. First, we'll get a user's last click and their historical clicks from the click log data.\n",
    "2. Next, we'll create the new features using the user's historical click data, the recall list, article information, and embedding vectors.\n",
    "3. Finally, we'll create the labels to form our supervised learning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953782f-bc34-4687-b755-d5637c85ceb8",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3260d-31a7-4497-9342-cb8341f28d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import logging\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293817f-3242-4f86-aad8-c321537329f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, use this cell to load data\n",
    "from google.colab import drive\n",
    "\n",
    "# Connect to Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define file paths\n",
    "data_path = '/content/drive/MyDrive/Datasets/news-rec-sys/'\n",
    "save_path = '/content/drive/MyDrive/Datasets/news-rec-sys/temp_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c9cde-c11d-432c-b5ba-545222649704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a local machine\n",
    "data_path = './data/' \n",
    "save_path = './data/temp_results/' # save temperary result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815f030-7d63-4e46-b284-f7ca89cf0517",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Splitting Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7246-8a99-4734-a864-a89f29689ce0",
   "metadata": {},
   "source": [
    "### 2.1 Function to save DataFrame memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b5927-2310-42d0-a2ce-5e04076362c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard function for memory optimization\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()  # Record the start time of the function\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  # List of numeric data types\n",
    "    start_mem = df.memory_usage().sum / 1024**2  # Calculate the memory usage of the DataFrame (in Mb)\n",
    "\n",
    "    # Iterate through each column of the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_types = df[col].dtypes  # Get the data type of the column\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()  # Get the minimum value in the column\n",
    "            c_max = df[col].max()  # Get the maximum value in the column\n",
    "\n",
    "            # Check if there are missing values in the minimum and maximum values\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "\n",
    "            # Choose the appropriate data type conversion based on the data type's range\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum / 1024**2  # Calculate the memory usage of the DataFrame after conversion (in Mb)\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction), time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                  100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                  (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ca6d-d2c9-49a6-bb8e-624df83b6bef",
   "metadata": {},
   "source": [
    "### 2.2 Training and Validation Split Function\n",
    "We split the data into training and validation sets to test our model's performance offline. To fully simulate the validation set, we will take a portion of the user data from the training set and use all of their information to form the validation set.\n",
    "\n",
    "The benefit of doing this split early is that it reduces the pressure of creating ranking features. Generating features for the entire dataset at once can be time-consuming, so this approach helps us work more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9d476-a69c-431b-a400-10491f2c34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_click_df refers to the training set.\n",
    "# sample_user_nums is the number of users to sample for the validation set.\n",
    "def trn_val_split(all_click_df, sample_user_nums):\n",
    "    all_click = all_click_df\n",
    "    all_user_ids = all_click['user_id'].unique()\n",
    "    \n",
    "    # replace=False means that users cannot be sampled more than once.\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=False)\n",
    "\n",
    "    click_val = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    click_trn = all_click[~all_click['user_id'].isin(sample_user_ids)]\n",
    "\n",
    "    # Extract the last click from the validation set to serve as the answer.\n",
    "    click_val = click_val.sort_values(['user_id', 'click_timestamp'])\n",
    "    val_ans = click_val.groupby('user_id').tail(1)\n",
    "\n",
    "    click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)\n",
    "\n",
    "    # Remove cases where a user in val_ans has only one click.\n",
    "    # If a user has only one click and it's put into val_ans,\n",
    "    # the training set will have no data for that user,\n",
    "    # leading to a user cold-start problem which complicates model validation.\n",
    "    val_ans = val_ans[val_ans['user_id'].isin(click_val['user_id'].unique())] # Ensure users in the answer set also appear in the validation set.\n",
    "    click_val = click_val[click_val['user_id'].isin(val_ans['user_id'].unique())]\n",
    "\n",
    "    return click_trn, click_val, val_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf55a8-0feb-4f46-9edb-9cddc2332833",
   "metadata": {},
   "source": [
    "### 2.3 Get Historical Clicks and Last Click Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657aacc5-0a2f-4276-a447-9844623ac690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical and last clicks from the current data\n",
    "def get_hist_and_last_click(all_click):\n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    # If the user has only one click record (len(user_df) == 1),\n",
    "    # the hist_func will return the entire click record for that user.\n",
    "    # Otherwise, it will return all clicks except the last one.\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb9a39-9c76-47f0-b1ad-91bc10938366",
   "metadata": {},
   "source": [
    "### 2.4 Read the Training, Validation, and Test Sets Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e151da1-5892-4dbd-bf0d-38402d898c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trn_val_tst_data(data_path, offline=True):\n",
    "    if offline:\n",
    "        click_trn_data = pd.read_csv(data_path + 'train_click_log.csv')  # Training Set\n",
    "        click_trn_data = reduce_mem(click_trn_data)\n",
    "        click_trn, click_val, val_ans = trn_val_split(click_trn_data, sample_user_nums)\n",
    "    else:\n",
    "        click_trn = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "        click_trn = reduce_mem(click_trn)\n",
    "        click_val = None\n",
    "        val_ans = None\n",
    "\n",
    "    click_tst = pd.read_csv(data_path + 'testA_click_log.csv')  # Test set\n",
    "\n",
    "    return click_trn, click_val, click_tst, val_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca94f6-642d-4589-b4eb-6c7d24587943",
   "metadata": {},
   "source": [
    "### 2.5 Load Recall Dictionary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97569e5-a793-4922-9948-fb5dd90159a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a multi-channel recall list or a single-channel recall list.\n",
    "def get_recall_list(save_path, single_recall_model=None, multi_recall=False):\n",
    "    if multi_recall:\n",
    "        return pickle.load(open(save_path + 'final_recall_items_dict.pkl', 'rb'))\n",
    "\n",
    "    if single_recall_model == 'i2i_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_recall_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'i2i_emb_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_emb_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'user_cf':\n",
    "        return pickle.load(open(save_path + 'youtubednn_usercf_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'youtubednn':\n",
    "        return pickle.load(open(save_path + 'youtube_u2i_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df3d7c-4155-41bb-9821-e20ccb667dde",
   "metadata": {},
   "source": [
    "### 2.6 Read Article Information Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f4af3-fd94-46a0-bcd2-06dfb32efae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info_df():\n",
    "    article_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    article_info_df = reduce_mem(article_info_df)\n",
    "\n",
    "    return article_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beba31e-1426-4f5d-95e6-740788f53b04",
   "metadata": {},
   "source": [
    "## 2.7 Read various Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df53b1d-bef3-406e-9db3-1556727b9715",
   "metadata": {},
   "source": [
    "### 2.7.1 Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c01905-8467-4e6f-9440-3914840c6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_item_word2vec(click_df, embed_size=64, save_name='item_w2v_emb.pkl', split_char=' '):\n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    # The data must be converted to strings before training.\n",
    "    click_df['click_article_id'] = click_df['click_article_id'].astype(str)\n",
    "    # Convert the data into a sentence-like format.\n",
    "    docs = click_df.groupby(['user_id'])['click_article_id'].apply(lambda x: list(x)).reset_index()\n",
    "    docs = docs['click_article_id'].values.tolist()\n",
    "\n",
    "    # Set up logging for easy monitoring of training progress.\n",
    "    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    \n",
    "    # The default negative sampling is 5.\n",
    "    ww2v = Word2Vec(docs, verctor_size=16, sg=1, window=5, seed=2020, workers=24, min_count=1, iter=10)\n",
    "\n",
    "    # Save the embeddings as a dictionary.\n",
    "    item_w2v_emb_dict = {k: w2v[k] for k in click_df['click_article_id']}\n",
    "    pickle.dump(item_w2v_emb_dict, open(save_path + 'item_w2v_emb.pkl', 'wb'))\n",
    "\n",
    "    return item_w2v_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f0737-6be7-4efb-beb4-24b1ee8e0f13",
   "metadata": {},
   "source": [
    "### 2.7.2 Load embeddings from the recall phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99e59d-7083-426b-8d89-f5cf80868d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec embedding and the three embedding in the recall phase.\n",
    "def get_embedding(save_path, all_click_df):\n",
    "    if os.path.exists(save_path + 'item_content_emb.pkl'):\n",
    "        item_content_emb_dict = pickle.load(open(save_path + 'item_content_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('item_content_emb.pkl file does not exist...')\n",
    "        \n",
    "    # The Word2Vec embeddings need to be pre-trained.\n",
    "    if os.path.exists(save_path + 'item_w2v_emb.pkl'):\n",
    "        item_w2v_emb_dict = pickle.load(open(save_path + 'item_w2v_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        item_w2v_emb_dict = train_item_word2vec(all_click_df)\n",
    "\n",
    "    if os.path.exists(save_path + 'item_youtube_emb.pkl'):\n",
    "        item_youtube_emb_dict = pickle.load(open(save_path + 'item_youtube_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('item_youtube_emb.pkl file does not exist...')\n",
    "\n",
    "    if os.path.exists(save_path + 'user_youtube_emb.pkl'):\n",
    "        user_youtube_emb_dict = pickle.load(open(save_path + 'user_youtube_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('user_youtube_emb.pkl file does not exist...')\n",
    "\n",
    "    return item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40393f-3f47-4751-a037-6d06147c7a5a",
   "metadata": {},
   "source": [
    "## 3. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de4c33-cf86-4f4f-9158-86bd5a4d11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distinction between 'offline' and 'online' here is whether the validation set is empty.\n",
    "click_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=False)\n",
    "\n",
    "click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)\n",
    "\n",
    "if click_val is not None:\n",
    "    click_val_hist, click_val_last = click_val, val_ans\n",
    "else:\n",
    "    click_val_hist, click_val_last = None, None\n",
    "\n",
    "click_tst_hist = click_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015421f-c309-4e97-8dc2-c0bcfd237b04",
   "metadata": {},
   "source": [
    "## 4. Downsampling the negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffe8ac-8096-46f9-9a02-187198326e32",
   "metadata": {},
   "source": [
    "After the recall phase, we're left with a list of positive items (what a user clicked) and a huge number of negative items (what they didn't click). This creates a massive class imbalance.\n",
    "\n",
    "#### The Problem: Imbalanced Data\n",
    "The data is extremely imbalanced. For each positive click (label 1), there are thousands of negative examples (label 0). Training a model on this skewed data would lead it to simply predict \"not clicked\" for everything, resulting in a useless model.\n",
    "\n",
    "#### The Solution: Downsampling\n",
    "To solve this, we downsample the negative samples. This means we selectively choose a smaller number of negative examples to create a more balanced ratio between positive and negative data points. This serves two purposes:\n",
    "\n",
    "- Mitigates Imbalance: It makes the ratio of positive to negative samples more manageable, allowing the model to learn what a \"click\" looks like rather than just what a \"non-click\" looks like.\n",
    "\n",
    "- Reduces Computation: It drastically reduces the size of the training dataset, which makes feature engineering and model training significantly faster.\n",
    "\n",
    "Key Considerations for Downsampling\n",
    "When performing negative downsampling, we follow these important rules:\n",
    "\n",
    "1. Only the negative samples are downsampled. All positive samples should be retained.\n",
    "2. After downsampling, ensure that all users and items that appeared in the original data are still present in the new, smaller dataset.\n",
    "3. The ratio of positive to negative samples is a hyperparameter we need to tune. We will try different ratios to see what works best for your model.\n",
    "4. Since downsampling changes the dataset, it's crucial to update our user recall lists to reflect the new data. This ensures that any subsequent features, especially those related to a user's position or history, are accurate.\n",
    "\n",
    "In this context, we're performing the negative downsampling early because creating ranking features for the full, imbalanced dataset would be too slow. This strategic step helps make the entire process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbb109-3285-4733-81bb-1aa104c5e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a recall list dictionary to a DataFrame format\n",
    "def recall_dict_2_df(recall_list_dict):\n",
    "    \"\"\"\n",
    "    Converts a recall list dictionary (recall_list_dict) to a DataFrame.\n",
    "    Parameters:\n",
    "        recall_list_dict (dict): A dictionary of recall lists, where keys are users\n",
    "                                 and values are lists of recalled items.\n",
    "    Returns:\n",
    "        recall_list_df (pd.DataFrame): The converted DataFrame, containing users, items, and scores.\n",
    "\n",
    "    \"\"\"\n",
    "    df_row_list = []  # [user, item, score]\n",
    "    for user, recall_list in tqdm(recall_list_dict.items()):\n",
    "        for item, score in recall_list:\n",
    "            df_row_list.append([user, item, score])\n",
    "\n",
    "    col_names = ['user_id', 'sim_item', 'score']\n",
    "    recall_list_df = pd.DataFrame(df_row_list, columns=col_names)\n",
    "\n",
    "    return recall_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44463345-9ee4-469c-9d2c-b04169ebb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The negative sampling function, where the sampling ratio can be controlled. A default value is provided here.\n",
    "def neg_sample_recall_data(recall_items_df, sample_rate=0.001):\n",
    "    pos_data = recall_items_df[recall_items_df['label'] == 1]\n",
    "    neg_data = recall_items_df[recall_items_df['label'] == 0]\n",
    "\n",
    "    print('pos_data_num:', len(pos_data), 'neg_data_num:', len(neg_data), 'pos/neg:', len(pos_data)/len(neg_data))\n",
    "\n",
    "    # Grouped sampling function\n",
    "    def neg_sample_func(group_df):\n",
    "        neg_num = len(group_df)\n",
    "        sample_num = max(int(neg_num * sample_rate), 1)  # Ensure at least one sample is taken.\n",
    "        sample_num = min(sample_num, 5)  # Ensure a maximum of 5 samples; this can be adjusted.\n",
    "        return group_df.sample(n=sample_num, replace=True)\n",
    "\n",
    "    # Perform negative sampling on a per-user basis, ensuring all users are in the sampled data.\n",
    "    neg_data_user_sample = neg_data.groupby('user_id', group_keys=False).apply(neg_sample_func)\n",
    "    # Perform negative sampling on a per-item basis, ensuring all items are in the sampled data.\n",
    "    neg_data_item_sample = neg_data.groupby('sim_item', group_keys=False).apply(neg_sample_func)\n",
    "\n",
    "    # Merge the sampled data from both of the above cases.\n",
    "    neg_data_new = neg_data_user_sample.append(neg_data_item_sample)\n",
    "    # Since the two operations above were separate, some data points might be duplicated.\n",
    "    # We must remove duplicates from the merged data.\n",
    "    neg_data_new = neg_data_new.sort_values(['user_id', 'score']).drop_duplicates(['user_id', 'sim_item'], keep='last')\n",
    "\n",
    "    # Merge with the positive samples.\n",
    "    data_new = pd.concat([pos_data, neg_data_new], ignore_index=True)\n",
    "\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8872fb1-997d-4452-8dc9-aa10ff777c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the recall data\n",
    "def get_rank_label_df(recall_list_df, label_df, is_test=False):\n",
    "    # The test set has no labels. To unify the code, we'll use a negative number as a placeholder.\n",
    "    if is_test:\n",
    "        recall_list_df['label'] = -1\n",
    "        return recall_list_df\n",
    "\n",
    "    label_df = label_df.rename(columns={'click_article_id': 'sim_item'})\n",
    "    recall_list_df_ = recall_list_df.merge(label_df[['user_id', 'sim_item', 'click_timestamp']], \\\n",
    "                                               how='left', on=['user_id', 'sim_item'])\n",
    "    recall_list_df_['label'] = recall_list_df_['click_timestamp'].apply(lambda x: 0.0 if np.isnan(x) else 1.0)\n",
    "    del recall_list_df_['click_timestamp']\n",
    "\n",
    "    return recall_list_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b530a1-b641-4254-8751-408649350ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recall_item_label_df(click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df):\n",
    "    # Get the recall list for the training data.\n",
    "    trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n",
    "    # Label the training data.\n",
    "    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n",
    "    # Perform negative sampling on the training data.\n",
    "    trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n",
    "\n",
    "    if click_val is not None:\n",
    "        val_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_val_hist['user_id'].unique())]\n",
    "        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=False)\n",
    "        val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)\n",
    "    else:\n",
    "        val_user_item_label_df = None\n",
    "\n",
    "    # Test data does not need negative sampling; all recalled items are directly given a label of -1.\n",
    "    tst_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_tst_hist['user_id'].unique())]\n",
    "    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, None, is_test=True)\n",
    "\n",
    "    return trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e37fb5-48ae-4963-8df3-d96a7274cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the recall list\n",
    "recall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf')\n",
    "# Convert the recall dictionary to a DataFrame\n",
    "recall_list_df = recall_dict_2_df(recall_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850e747-7153-471a-bf66-62f5aa6fff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the training and validation data, and perform negative sampling.\n",
    "trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist,\n",
    "                                                                                                       click_val_hist,\n",
    "                                                                                                       click_tst_hist,\n",
    "                                                                                                       click_trn_last,\n",
    "                                                                                                       click_val_last,\n",
    "                                                                                                       recall_list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37af37-d50a-4f2b-894d-3d866b7eaf2a",
   "metadata": {},
   "source": [
    "## 5. Convert the Downsampled Recall Data to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b9f2a-caa9-41b1-91b9-496230f50f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the final recall DataFrame to a dictionary format for ranking feature creation.\n",
    "def make_tuple_func(group_df):\n",
    "    row_data = []\n",
    "    for name, row_df in group_df.iterrows():\n",
    "        row_data.append((row_df['sim_item'], row_df['score'], row_df['label']))\n",
    "\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf8b9e-b794-40ff-8c01-0d81b517515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_label_tuples = trn_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "trn_user_item_label_tuples_dict = dict(zip(trn_user_item_label_tuples['user_id'], trn_user_item_label_tuples[0]))\n",
    "\n",
    "if val_user_item_label_df is not None:\n",
    "    val_user_item_label_tuples = val_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "    val_user_item_label_tuples_dict = dict(zip(val_user_item_label_tuples['user_id'], val_user_item_label_tuples[0]))\n",
    "else:\n",
    "    val_user_item_label_tuples_dict = None\n",
    "\n",
    "tst_user_item_label_tuples = tst_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "tst_user_item_label_tuples_dict = dict(zip(tst_user_item_label_tuples['user_id'], tst_user_item_label_tuples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9105b-a34b-4603-a0b5-92a26e4ae9bf",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dfbef8-49e2-4377-8ec0-178f33870460",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating new features from raw data to improve the performance of machine learning models. Here's a summary of the key techniques:\n",
    "\n",
    "#### a. Text Feature Engineering\n",
    "- **Text Preprocessing**: Clean and normalize text data by tokenizing, removing stop words, removing irrelevant characters, symbols, or punctuation, removing duplicated/noisy/missing data, performing stemming or lemmatization, and handling named entities.\n",
    "- **Text Length Features**: Create features based on the length of a news article's title or body, such as character count, word count, or sentence count.\n",
    "- **Sentiment Analysis**: Use sentiment analysis models (e.g., VADER, TextBlob) to extract the emotional tone of an article as a feature.\n",
    "\n",
    "#### b. User Behavior Features\n",
    "- **Time Interval Features**: Calculate the time gap between user actions, such as the interval between clicks on two different articles.\n",
    "- **User Behavior Sequence**: Sequence a user's historical actions and use a sequence model (e.g., RNN, LSTM) to extract features.\n",
    "- **User Interest Tags**: Use clustering or topic modeling to build interest tags that represent a user's preferences for different topics or categories.\n",
    "\n",
    "#### c. Article Attribute Features\n",
    "- **Topic Modeling**: Use models like LDA or LSA to identify and use the topics of articles as features.\n",
    "- **One-Hot Encoding**: Convert an article's category into a numerical feature using one-hot encoding.\n",
    "- **Title Keywords**: Extract keywords from article titles to use as features.\n",
    "\n",
    "#### d. Contextual Features\n",
    "- **Geographic Features**: Use a user's location or an article's location tags as features.\n",
    "- **Time Features**: Convert timestamps into features like season, weekday/weekend, or holiday.\n",
    "- **User Device Features**: Use information about a user's device, operating system, or browser type.\n",
    "\n",
    "#### e. Embedding and Dimensionality Reduction\n",
    "- **Word2Vec Embeddings**: Use a Word2Vec model (or a transformer-based one such as BERT) to convert article titles or bodies into low-dimensional, dense vectors.\n",
    "- **TF-IDF Weighted Dimensionality Reduction**: Reduce the dimensionality of TF-IDF weighted features to remove noise and make them more manageable.\n",
    "\n",
    "#### f. Feature Crossing and Combination\n",
    "- **Feature Combination**: Combine multiple features, such as a user's interest tag with an article's category.\n",
    "- **Feature Crossing**: Perform feature crossing for categorical features, such as combining a user's click count for a certain category with an article's category.\n",
    "\n",
    "#### g. Handling Missing Values and Outliers\n",
    "- **Missing Value Imputation**: Fill in missing feature values with methods like the mean, median, or mode.\n",
    "- **Outlier Treatment**: Detect and handle outliers using methods like the Z-Score or box plots.\n",
    "\n",
    "#### h. Feature Selection and Importance\n",
    "- **Variance Thresholding**: Remove features with a low variance, assuming they have little predictive power.\n",
    "- **Correlation Analysis**: Calculate the correlation between features and the target variable to select the most relevant ones.\n",
    "- **Feature Importance Ranking**: Use a machine learning model (e.g., Random Forest, GBDT) to rank features by their importance and select the most significant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa823ad6-07ec-4d96-a1a3-349904de7877",
   "metadata": {},
   "source": [
    "### 6.1 Feature Engineering of User Click History Versus Recalled Articles\n",
    "For each user and each candidate article they've been recalled, we need to generate a set of features that capture the relationship between the candidate article and the user's past interests.\n",
    "\n",
    "The specific steps are as follows:\n",
    "\n",
    "1. Retrieve Recent Clicks: For every user, get the item_ids of the last N articles they clicked.\n",
    "2. Generate Candidate-Specific Features: For each candidate article, calculate the following features by comparing it to the user's last N clicks:\n",
    "\n",
    "- **Similarity Statistics**: Compute statistical features (sum, max, min, mean) of the similarity between the candidate article and the user's last N clicked articles (e.g., using embedding similarity).\n",
    "\n",
    "- **Time-Based Features**: Calculate the time difference between the candidate article's creation time and the creation times of the last N clicked articles.\n",
    "\n",
    "- **Length Features**: Determine the word count difference between the candidate article and the last N clicked articles.\n",
    "\n",
    "- **User Similarity**: Calculate the similarity score between the user and the candidate article (e.g., using a YouTube DNN model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830ba75-af33-40a1-827b-ba9425aacd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create history-related features based on the data below.\n",
    "def create_feature(users_id, recall_list, click_hist_df,  articles_info, articles_emb, user_emb=None, N=1):\n",
    "    \"\"\"\n",
    "    Creates features based on a user's historical behavior.\n",
    "    :param users_id: A list of user IDs.\n",
    "    :param recall_list: A dictionary where keys are user IDs and values are lists of candidate articles recalled for that user.\n",
    "    :param click_hist_df: DataFrame containing the user's historical click information.\n",
    "    :param articles_info: DataFrame with information about articles.\n",
    "    :param articles_emb: A dictionary of article embedding vectors. This can be item_content_emb, item_w2v_emb, or item_youtube_emb.\n",
    "    :param user_emb: A dictionary of user embedding vectors (user_youtube_emb). If not provided, the feature will not be created.\n",
    "                     Note that if user_emb is used, articles_emb must be item_youtube_emb to ensure the dimensions match.\n",
    "    :param N: The number of the most recent clicks to consider. Default is 1 because many users in the test set have only one historical click, which prevents null values.\n",
    "    \"\"\"\n",
    "    # Create a 2D list to store the results, which will later be converted to a DataFrame.\n",
    "    all_user_feas = []\n",
    "    i = 0\n",
    "    for user_id in tqdm(users_id):\n",
    "        # Get the user's last N clicks.\n",
    "        hist_user_items = click_hist_df[click_hist_df['user_id']==user_id]['click_article_id'][-N:]\n",
    "\n",
    "        # Iterate through the user's recall list.\n",
    "        for rank, (article_id, score, label) in enumerate(recall_list[user_id]):\n",
    "            # Get the article's creation time and word count.\n",
    "            a_create_time = articles_info[articles_info['article_id']==article_id]['created_at_ts'].values[0]\n",
    "            a_words_count = articles_info[articles_info['article_id']==article_id]['words_count'].values[0]\n",
    "            single_user_fea = [user_id, article_id]\n",
    "            # Calculate the sum, max, min, and mean of similarity, time difference, and word difference\n",
    "            sim_fea = []\n",
    "            time_fea = []\n",
    "            word_fea = []\n",
    "            # Iterate through the user's last N clicked articles.\n",
    "            for hist_item in hist_user_items:\n",
    "                b_create_time = articles_info[articles_info['article_id']==hist_item]['created_at_ts'].values[0]\n",
    "                b_words_count = articles_info[articles_info['article_id']==hist_item]['words_count'].values[0]\n",
    "\n",
    "                sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))\n",
    "                time_fea.append(abs(a_create_time - b_create_time))\n",
    "                word_fea.append(abs(a_words_count - b_words_count))\n",
    "\n",
    "            single_user_fea.extend(sim_fea)   # Similarity features\n",
    "            single_user_fea.extend(time_fea)   # Time difference features\n",
    "            single_user_fea.extend(word_fea)   # Word count difference features\n",
    "            single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea)])  # ç›¸ä¼¼æ€§çš„ç»Ÿè®¡ç‰¹å¾\n",
    "\n",
    "            if user_emb:  # If user embedding is provided, calculate the similarity feature between the recalled article and the user.\n",
    "                single_user_fea.append(np.dot(user_emb[user_id], articles_emb[article_id]))\n",
    "\n",
    "            single_user_fea.extend([score, rank, label])\n",
    "            # Add to the main list.\n",
    "            all_user_feas.append(single_user_fea)\n",
    "\n",
    "    # Define column names.\n",
    "    id_cols = ['user_id', 'click_article_id']\n",
    "    sim_cols = ['sim' + str(i) for i in range(N)]\n",
    "    time_cols = ['time_diff' + str(i) for i in range(N)]\n",
    "    word_cols = ['word_diff' + str(i) for i in range(N)]\n",
    "    sta_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean']\n",
    "    user_item_sim_cols = ['user_item_sim'] if user_emb else []\n",
    "    user_score_rank_label = ['score', 'rank', 'label']\n",
    "    cols = id_cols + sim_cols + time_cols + word_cols + sta_cols + user_item_sim_cols + user_score_rank_label\n",
    "\n",
    "    # Convert to a DataFrame.\n",
    "    df = pd.DataFrame(all_user_feas, columns=cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac334861-1fba-4cf6-8dcd-21b1d7c904e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article info\n",
    "article_info_df = get_article_info_df()\n",
    "\n",
    "# Merge Log data, which is all the previous data\n",
    "if click_val is not None:\n",
    "    all_click = click_trn.append(click_val)\n",
    "all_click = click_trn.append(click_tst)\n",
    "all_click = reduce_mem(all_click)\n",
    "\n",
    "# Get embeddings\n",
    "item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict = get_embedding(save_path, all_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce08d6-a3a3-4d21-9cee-a27ce84965e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features related to the recalled articles in the training, validation, and test data.\n",
    "trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n",
    "                                            click_trn_hist, article_info_df, item_content_emb_dict)\n",
    "trn_user_item_feats_df.to_csv(save_path + 'trn_user_item_feats_df.csv', index=False)\n",
    "\n",
    "if val_user_item_label_tuples_dict is not None:\n",
    "    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \\\n",
    "                                            click_val_hist, article_info_df, item_content_emb_dict)\n",
    "    val_user_item_feats_df.to_csv(save_path + 'val_user_item_feats_df.csv', index=False)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "tst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \\\n",
    "                                            click_tst_hist, article_info_df, item_content_emb_dict)\n",
    "tst_user_item_feats_df.to_csv(save_path + 'tst_user_item_feats_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c46a7-571d-4287-b533-6a2ab6ea4910",
   "metadata": {},
   "source": [
    "### 6.2 User Behavior and Inherent Article Features\n",
    "We will combine our existing features with more features in this section. Here's a breakdown of the features we will use:\n",
    "\n",
    "#### 1. Inherent Article Features\n",
    "- **Article Word Count**: The number of words in an article indicates its length. This helps us understand a user's preference for long-form vs. short-form content.\n",
    "\n",
    "- **Article Creation Time**: An article's creation timestamp reflects its freshness. This can be used to gauge a user's preference for timely, up-to-date news.\n",
    "\n",
    "- **Article Embedding**: Converting articles into low-dimensional, dense vectors (e.g., via Word2Vec or BERT) helps us capture the semantic similarity between articles, which is crucial for improving recommendation accuracy.\n",
    "\n",
    "#### 2. User Click Environment Features\n",
    "- **Device Information**: Features related to a user's device type, operating system, or browser can reflect their habits and preferences, helping us deliver personalized content for their specific environment.\n",
    "\n",
    "#### 3. Users Behavior and Article Popularity Features\n",
    "- **User Activity**: We can create features that reflect a user's activeness by counting their total clicks and analyzing the time intervals between their clicks.\n",
    "\n",
    "- **Article Popularity**: We'll build features to measure an article's popularity or \"hotness\" by analyzing its total click count and the distribution of clicks over time.\n",
    "\n",
    "- **User Timeliness Preference**: By comparing the click times of a user's historical articles to their creation times, we can understand their preference for content timeliness and recommend articles accordingly.\n",
    "\n",
    "- **User Topic Preferences**: We can build features that represent a user's favorite topics by statistically analyzing the categories of their past clicks. This helps us predict if a new article belongs to a topic they've previously engaged with.\n",
    "\n",
    "- **User Reading Length Preference**: By analyzing the word counts of a user's historical articles (e.g., calculating the average word count), we can create a feature that reflects their preference for a specific article length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba215b-b212-4f04-b5cc-55d1559f0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with article information\n",
    "all_data = all_click.merge(article_info_df, left_on='click_article_id', right_on='article_id')\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8b4d3-d1e6-441e-8c03-b0748fbcf22a",
   "metadata": {},
   "source": [
    "### 6.2.1 User Activity: Total Clicks and Time Intervals between clicks\n",
    "To distinguish active users from less active ones, we'll create a feature that combines their click frequency and the time between their clicks. The steps for this feature are as follows:\n",
    "\n",
    "1. Group by User: First, we group the data by user_id.\n",
    "\n",
    "2. Calculate Metrics: For each user, we calculate two key metrics:\n",
    "\n",
    " - The total number of articles they've clicked.\n",
    "\n",
    " - The average time interval between their consecutive clicks.\n",
    "\n",
    "3. Combine and Normalize: To create a single score, we take the inverse of the click count and normalize it. We then do the same for the normalized average time interval. These two normalized values are then added together.\n",
    "\n",
    "4. Handle Single Clicks: If a user has only one click, the average time interval will be a null value. For these cases, we'll assign a large, distinct value to ensure they are properly separated from other users in our analysis.\n",
    "\n",
    "The intuition behind this feature is that a smaller final value indicates a more active userâ€”meaning they clicked more often (high count -> low inverse) and did so in shorter time frames (short interval -> low normalized value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba3088-ba5b-41d6-8b02-a47103fd8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_level(all_data, cols):\n",
    "    \"\"\"\n",
    "    Creates features to distinguish user activity levels.\n",
    "    :param all_data: The dataset.\n",
    "    :param cols: The feature columns to use.\n",
    "    \"\"\"\n",
    "    data = all_data[cols]\n",
    "    data.sort_values(['user_id', 'click_timestamp'], inplace=True)\n",
    "    user_act = pd.DataFrame(data.groupby('user_id', as_index=False)[['click_article_id', 'click_timestamp']].\\\n",
    "                            agg({'click_article_id':np.size, 'click_timestamp': {list}}).values, columns=['user_id', 'click_size', 'click_timestamp'])\n",
    "\n",
    "    # Calculate the mean time difference.\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "\n",
    "    user_act['click_time_diff_mean'] = user_act['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "\n",
    "    # Take the inverse of the click count.\n",
    "    user_act['click_size'] = 1 / user_act['click_size']\n",
    "\n",
    "    # Normalize both features.\n",
    "    user_act['click_size'] = (user_act['click_size'] - user_act['click_size'].min()) / (user_act['click_size'].max() - user_act['click_size'].min())\n",
    "    user_act['click_time_diff_mean'] = (user_act['click_time_diff_mean'] - user_act['click_time_diff_mean'].min()) / (user_act['click_time_diff_mean'].max() - user_act['click_time_diff_mean'].min())\n",
    "    user_act['active_level'] = user_act['click_size'] + user_act['click_time_diff_mean']\n",
    "\n",
    "    user_act['user_id'] = user_act['user_id'].astype('int')\n",
    "    del user_act['click_timestamp']\n",
    "\n",
    "    return user_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1d1dd-74cb-439e-8756-a9e79fbac8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_act_fea = active_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])\n",
    "user_act_fea.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eacdf7-2320-4b62-8638-54e280af4b61",
   "metadata": {},
   "source": [
    "### 6.2.2 Article Popularity\n",
    "To measure an article's popularity, we'll use a similar approach to how we measured user activity. The logic is that an article is considered \"hot\" if it receives many clicks in a short amount of time. Here are the steps for creating this feature:\n",
    "\n",
    "1. Group by Article: We group the data by click_article_id.\n",
    "\n",
    "2. Calculate Metrics: For each article, we calculate two metrics:\n",
    "\n",
    " - The total number of times it has been clicked.\n",
    "\n",
    " - The average time interval between each of those clicks.\n",
    "\n",
    "3. Combine and Normalize: To get a single \"hotness\" score, we take the inverse of the click count and normalize it. The average time interval will also be normalized. We add these two normalized values together.\n",
    "\n",
    "The result is a single score where a smaller value indicates a hotter article. This is because a lower value means the article was clicked more often and in shorter time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6c3c2-c910-47e4-98ca-d1911f39ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_level(all_data, cols):\n",
    "    \"\"\"\n",
    "    Creates features to measure an article's popularity.\n",
    "    :param all_data: The dataset.\n",
    "    :param cols: The feature columns to use.\n",
    "    \"\"\"\n",
    "    data = all_data[cols]\n",
    "    data.sort_values(['click_article_id', 'click_timestamp'], inplace=True)\n",
    "    article_hot = pd.DataFrame(data.groupby('click_article_id', as_index=False)[['user_id', 'click_timestamp']].\\\n",
    "                               agg({'user_id':np.size, 'click_timestamp': {list}}).values, columns=['click_article_id', 'user_num', 'click_timestamp'])\n",
    "\n",
    "    # Calculate the mean time difference between clicks.\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "\n",
    "    article_hot['article_time_diff_mean'] = article_hot['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "\n",
    "    # Take the inverse of the click count.\n",
    "    article_hot['user_num'] = 1 / article_hot['user_num']\n",
    "\n",
    "    # Normalize both features.\n",
    "    article_hot['user_num'] = (article_hot['user_num'] - article_hot['user_num'].min()) / (article_hot['user_num'].max() - article_hot['user_num'].min())\n",
    "    article_hot['article_time_diff_mean'] = (article_hot['article_time_diff_mean'] - article_hot['article_time_diff_mean'].min()) / (article_hot['article_time_diff_mean'].max() - article_hot['article_time_diff_mean'].min())\n",
    "    article_hot['hot_level'] = article_hot['user_num'] + article_hot['time_diff_mean']\n",
    "\n",
    "    article_hot['click_article_id'] = article_hot['click_article_id'].astype('int')\n",
    "\n",
    "    del article_hot['click_timestamp']\n",
    "\n",
    "    return article_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38871b-27a1-4434-843a-55b8412f363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_hot_fea = hot_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])\n",
    "article_hot_fea.to_csv(save_path + 'articles_hot_features.csv', index=False)\n",
    "article_hot_fea.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825ab34-8f3c-4eeb-a5f0-bb615e30bab8",
   "metadata": {},
   "source": [
    "### 6.3 User Habit and Preference Features\n",
    "To create a more comprehensive user profile, we can build a DataFrame that contains features reflecting their unique habits and preferences. Here are some key features to construct:\n",
    "\n",
    "- **User Device Habits**: Identify a user's most frequently used device (the mode).\n",
    "\n",
    "- **User Time Habits**: Analyze the timestamps of a user's clicks to understand when they are most active. We can extract the hour of the day or day of the week and find a mean or most common value.\n",
    "\n",
    "- **User Topic Preferences**: Determine a user's content interests by statistically analyzing the topics of their historical clicks. This could be represented using multi-hot encoding to show which categories a user is interested in.\n",
    "\n",
    "- **User Reading Length Preferences**: Analyze the word counts of a user's historical articles to understand their preference for article length. We can calculate the average word count to see if they prefer long-form or short-form content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47310827-357a-445e-8e91-731bf7ab25d9",
   "metadata": {},
   "source": [
    "### 6.3.1 User Device Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cdafc-9f25-4925-83ff-a0d98f6f42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    Creates features for user devices.\n",
    "    :param all_data: The dataset.\n",
    "    :param cols: The feature columns to use.\n",
    "    \"\"\"\n",
    "    user_device_info = all_data[cols]\n",
    "\n",
    "    # Use the mode to represent the device information for each user.\n",
    "    user_device_info = user_device_info.groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "    return user_device_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81426484-f2ba-4786-afc8-b43fb1851998",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "user_device_info = device_fea(all_data, device_cols)\n",
    "user_device_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e09913-f034-4374-8acb-53c4fef3b76b",
   "metadata": {},
   "source": [
    "### 6.3.2 User Time Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08088da-5af4-433c-a487-b58ababdee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_time_hab_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    Creates features for user time habits.\n",
    "    :param all_data: The dataset.\n",
    "    :param cols: The feature columns to use.\n",
    "    \"\"\"\n",
    "    user_time_hab_info = all_data[cols]\n",
    "\n",
    "    # First, normalize the timestamps.\n",
    "    mm = MinMaxScaler()\n",
    "    user_time_hab_info['click_timestamp'] = mm.fit_transform(user_time_hab_info[['click_timestamp']])\n",
    "    user_time_hab_info['created_at_ts'] = mm.fit_transform(user_time_hab_info[['created_at_ts']])\n",
    "\n",
    "    user_time_hab_info = user_time_hab_info.groupby('user_id').agg('mean').reset_index()\n",
    "\n",
    "    user_time_hab_info.rename(columns={'click_timestamp': 'user_time_hob1', 'created_at_ts': 'user_time_hob2'}, inplace=True)\n",
    "    return user_time_hab_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087beabc-f47a-4aeb-9c08-1aad2e446fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_time_hab_cols = ['user_id', 'click_timestamp', 'created_at_ts']\n",
    "user_time_hab_info = user_time_hab_fea(all_data, user_time_hab_cols)\n",
    "user_time_hab_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7e16d-2dd2-4db7-9404-d4907785f705",
   "metadata": {},
   "source": [
    "### 6.3.3 User Topic Preference\n",
    "First, we will convert the topics of the articles a user has clicked into a list. During the final data aggregation, we will create a separate feature where an article's topic gets a value of 1 if it's in this list, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66295ff-8b1b-4092-a611-cacf93ed5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cat_hab_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    User's topic preferences.\n",
    "    :param all_data: The dataset.\n",
    "    :param cols: The feature columns to use.\n",
    "    \"\"\"\n",
    "    user_category_hab_info = all_data[cols]\n",
    "    user_category_hab_info = user_category_hab_info.groupby('user_id').agg({list}).reset_index()\n",
    "\n",
    "    user_cat_hab_info = pd.DataFrame()\n",
    "    user_cat_hab_info['user_id'] = user_category_hab_info['user_id']\n",
    "    user_cat_hab_info['cate_list'] = user_category_hab_info['category_id']\n",
    "\n",
    "    return user_cat_hab_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32380fb9-8f21-485d-a9f2-3ef12f310625",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_category_hab_cols = ['user_id', 'category_id']\n",
    "user_cat_hab_info = user_cat_hab_fea(all_data, user_category_hab_cols)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fdf26-e21a-4585-9c3c-a2cce4e72659",
   "metadata": {},
   "source": [
    "### 6.3.4 User Reading Length Preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effe6c4-a68e-4aab-b204-08486c7c660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_wcou_info = all_data.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "user_wcou_info.rename(columns={'words_count': 'words_hab'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33561857-de8e-4cfa-892e-5102ae94bdc5",
   "metadata": {},
   "source": [
    "### 6.4 Merge All User Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc1549c-d28c-4dbd-a1bc-8594e794bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "user_info = pd.merge(user_act_fea, user_device_info, on='user_id')\n",
    "user_info = user_info.merge(user_time_hab_info, on='user_id')\n",
    "user_info = user_info.merge(user_cat_hab_info, on='user_id')\n",
    "user_info = user_info.merge(user_wcou_info, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ae7db-7319-46ea-ab17-2c835ff18a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save user features so that they can be read directly in the future\n",
    "user_info.to_csv(save_path + 'user_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df5a71-e85c-4b0a-886c-15b3f4c936ed",
   "metadata": {},
   "source": [
    "## 7. Read Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31b0d3-2c9b-4d11-9a60-fb0bd9d34280",
   "metadata": {},
   "source": [
    "### 7.1 Read User-related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64775d96-e15b-4dae-b7e1-deb7692a9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = pd.read_csv(save_path + 'user_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc83b46-038f-46eb-a09a-b7f5199d1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(save_path + 'trn_user_item_feats_df.csv'):\n",
    "    trn_user_item_feats_df = pd.read_csv(save_path + 'trn_user_item_feats_df.csv')\n",
    "\n",
    "if os.path.exists(save_path + 'tst_user_item_feats_df.csv'):\n",
    "    tst_user_item_feats_df = pd.read_csv(save_path + 'tst_user_item_feats_df.csv')\n",
    "\n",
    "if os.path.exists(save_path + 'val_user_item_feats_df.csv'):\n",
    "    val_user_item_feats_df = pd.read_csv(save_path + 'val_user_item_feats_df.csv')\n",
    "else:\n",
    "    val_user_item_feats_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b78b38-b2b8-4203-9ce4-19cbd573a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train, validation, and test sets with user-related features\n",
    "# Below is for offline validation\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(user_info, on='user_id', how='left')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(user_info, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230e22c-0b4f-4f32-a899-7490d79edf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb42c7-5b19-4955-bc99-99845b3b43ab",
   "metadata": {},
   "source": [
    "### 7.2 Read Article-related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2796d-e256-4adb-a0ad-d04e46d8621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article info\n",
    "articles = get_article_info_df()\n",
    "articles_hot_fea =  pd.read_csv(data_path + 'articles_hot_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58847f-2443-4cf8-abbe-0571fcd7a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge article information with article popularity features\n",
    "articles.merge(articles_hot_fea, on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90798b8a-2f20-4298-b571-02c3ed1688a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train, validation, and test sets with article-related features\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(articles_hot_fea, on='click_article_id', how='left')\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(articles_hot_fea, on='click_article_id', how='left')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(articles_hot_fea, on='click_article_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb21cbe-c7c8-434f-b476-8d3d8d1e2722",
   "metadata": {},
   "source": [
    "### 7.3 Check if the Topic of the Recalled Article is in the User's Interest History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a0bec-8db5-4a26-982c-2499a19a9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df['is_cat_hab'] = trn_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df['is_cat_hab'] = val_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "    \n",
    "tst_user_item_feats_df['is_cat_hab'] = tst_user_item_feats_df.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad6a60-be4b-4f3d-9e66-1ce1ab09126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For offline validation\n",
    "del trn_user_item_feats_df['cate_list']\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    del val_user_item_feats_df['cate_list']\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "del tst_user_item_feats_df['cate_list']\n",
    "\n",
    "del trn_user_item_feats_df['article_id']\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    del val_user_item_feats_df['article_id']\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "del tst_user_item_feats_df['article_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3398a07-ade0-463a-b3fb-267715803622",
   "metadata": {},
   "source": [
    "## 8. Save The Features for the User's Recall list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b4334-50a8-4243-a791-a083527704c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features for training, validation, testing sets\n",
    "trn_user_item_feats_df.to_csv(save_path + 'trn_user_item_feats_df.csv', index=False)\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df.to_csv(save_path + 'val_user_item_feats_df.csv', index=False)\n",
    "    \n",
    "tst_user_item_feats_df.to_csv(save_path + 'tst_user_item_feats_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d29d8-fd0e-404e-b10e-9fdf18afba8d",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865d3b9-ba38-4191-a335-75a9fcd276be",
   "metadata": {},
   "source": [
    "\"Data and features determine the ceiling of a model's performance; the algorithm only gets you close to that ceiling.\" The quality of the features often determines the final outcome.\n",
    "\n",
    "In this section, we accomplished two major goals:\n",
    "\n",
    "- Framing the Problem: We transformed our recall results into a supervised learning dataset by creating features and assigning labels. This converted our prediction problem into a format a model can easily understand and learn from.\n",
    "\n",
    "- Building a Rich Feature Set: We've begun to create a series of features based on both user profiles and article profiles. These features go beyond raw data to capture nuanced information about user activity, preferences, and the characteristics of the content they engage with.\n",
    "\n",
    "We also tackled the data imbalance problem by implementing negative sampling, which ensures our model gets a balanced view of both what users like and what they don't."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
