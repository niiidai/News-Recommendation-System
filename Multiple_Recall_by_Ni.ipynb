{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df963ce-2bc5-4a00-a0e7-e28516a5dd4c",
   "metadata": {},
   "source": [
    "# ðŸ“° News Recommendation System Part 2 - Multi-Channel Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac1f86-7965-4198-85e0-b8ce0d1e889c",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Project Introduction\n",
    "This project explores user behavior prediction in a news recommendation scenario. The goal is to build a model that can predict a user's future click behavior based on their historical browsing and clicking behavior data, specifically the last news article they clicked on.\n",
    "\n",
    "The setting is inspired by a real-world news app, where delivering timely, relevant content is essential for user engagement. This project aims to simulate a practical recommender system, combining business intuition with machine learning techniques to address a realistic problem in the content recommendation space.\n",
    "\n",
    "## ðŸ“Š Data Overview\n",
    "The dataset contains user interaction data from a large-scale news platform, including:\n",
    "- 300,000 users\n",
    "- ~3 million clicks\n",
    "- 360,000+ unique news articles; each news article is represented by a pre-trained embedding vector, capturing semantic relationships between articles.\n",
    "\n",
    "We extracted click log data from 200,000 users as the training set, 50,000 users as test set A, and 50,000 users as test set B.\n",
    "\n",
    "## ðŸ“„ Data Tables\n",
    "\n",
    "- train_click_log.csv: Training set user click logs\n",
    "- testA_click_log.csv: Test set user click logs\n",
    "- articles.csv: News article information data table\n",
    "- articles_emb.csv: Embedding vector representation of news articles\n",
    "\n",
    "|        **Field**        |         **Description**          |\n",
    "| :---------------------: | :------------------------------: |\n",
    "|         user_id         |              User ID             |\n",
    "|    click_article_id     |            Clicked article ID    |\n",
    "|     click_timestamp     |            Click timestamp        |\n",
    "|    click_environment    |             Click environment     |\n",
    "|    click_deviceGroup    |            Click device group     |\n",
    "|        click_os         |           Click operating system  |\n",
    "|      click_country      |             Click city            |\n",
    "|      click_region       |             Click region          |\n",
    "|   click_referrer_type   |           Click source type       |\n",
    "|       article_id        | Article ID, corresponding to click_article_id |\n",
    "|       category_id       |            Article type ID        |\n",
    "|      created_at_ts      |          Article creation timestamp |\n",
    "|       words_count       |             Article word count     |\n",
    "| emb_1,emb_2,...,emb_249 |      Article embedding vector representation |\n",
    "\n",
    "## ðŸ“ Evaluation Metrics\n",
    "The final recommendation for each user will include five recommended articles, sorted by click probability.\n",
    "\n",
    "For example, for user1, our recommendation would be:\n",
    "> user1, article1, article2, article3, article4, article5.\n",
    "\n",
    "There is only one correct answer for each user's last clicked article, so we check if any of the recommended five articles match the actual answer. We will use **mean reciprocal rank** as the evaluation metric. The formula is as follows:\n",
    "$$\n",
    "score(user) = \\sum_{k=1}^5 \\frac{s(user, k)}{k}\n",
    "$$\n",
    "\n",
    "If article1 is the actual article clicked by the user, then s(user1, 1) = 1, and s(user1, 2-4) are all 0. If article2 is the article clicked by the user, then s(user, 2) = 1/2, and s(user, 1, 3, 4, 5) are all 0. Thus, score(user) = the reciprocal of the rank at which the match occurs. If there are no matches, score(user1) = 0. This is reasonable because we want hits to be as high-ranking as possible, which yields a higher score.\n",
    "\n",
    "## ðŸ’¡ Project Understanding\n",
    "The goal of this project is to **predict the last news article a user clicked, based on their historical browsing data**. Unlike traditional structured prediction problems, this is more aligned with real-world recommendation systems, using raw user click logs rather than neatly labeled data.\n",
    "\n",
    "To approach this, I framed the task as a **supervised learning** problem by transforming user-article interactions into \"features + labels\" training data. The core idea is to predict the likelihood of a user clicking a given article, turning this into a click-through rate (CTR) prediction task. This reframing allows for the use of **classification models**â€”starting with simple baselines like logistic regression and moving toward deep learning approaches.\n",
    "\n",
    "Now, we have converted this problem into a classification problem, where the classification label is whether the user will click on a particular article. The features of the classification problem will include the user and the article. We need to train a classification model to predict the probability of a particular user clicking on a specific article. This raises several additional questions:\n",
    "- How to create training and testing datasets?\n",
    "- What specific features can we leverage?\n",
    "- What models can we attempt?\n",
    "- With 360,000 articles and over 200,000 users, what strategies do we have to reduce the problem's scale? How do we make the final predictions?\n",
    "\n",
    "**For the second part, we will use a multi-channel recall strategy to create a set of candidate news articles for the recommendation system. This approach is also designed to solve the cold-start problem for new users.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ade7b-38a9-4823-bbb5-e58727add40b",
   "metadata": {},
   "source": [
    "Multi-channel recall is a powerful strategy that uses several different methods to generate a diverse pool of relevant content.\n",
    "- **Multiple Strategies**: The system uses a variety of simple, fast methods to find potential recommendations. Each method, or \"channel,\" looks at the data from a different angle. For example, one channel might find articles similar to what the user has clicked on before, while another might find the most popular articles overall.\n",
    "- **Balancing Speed and Quality**: These channels are designed to be fast, so the system can quickly find a large number of candidates. This keeps the recommendation process from being slow. By using many diverse channels, the system ensures a **high recall rate**, meaning it's very likely to find the most relevant items, which then perform well in the final ranking stage.\n",
    "- **Efficiency**: Because each channel operates independently, they can all run at the same time using multi-threading. This parallel processing significantly speeds up the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a10068-37f1-4369-836f-c745815a4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following packages if not installed\n",
    "!pip install faiss-cpu\n",
    "!pip install deepmatch\n",
    "!pip install deepctr\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a7d87-7374-4521-9347-4bb2b34224ca",
   "metadata": {},
   "source": [
    "1. **Faiss** (faiss-cpu): a library for efficient similarity search and clustering of dense vectors, allowing you to quickly find the **\"nearest neighbors\"** of a given vector. The 'faiss-cpu' package is a CPU-based version of Faiss\n",
    "\n",
    "- Key Feature: Faiss provides a variety of fast search algorithms that are optimized for large-scale nearest neighbor search tasks in high-dimensional vector spaces.\n",
    "\n",
    "- Best For: When you need to find similar items from massive datasets, such as searching for images, articles, or products that are similar to a user's preferences. It's a go-to for building the recall stage of a recommendation system.\n",
    "\n",
    "2. **DeepMatch**: a library for building deep learning-based matching models in recommendation systems. It's designed to solve the **\"matching\"** problemâ€”finding relevant items from a large pool that a user might be interested in.\n",
    "\n",
    "- Key Feature: The library provides ready-to-use deep learning models for matching users to items, often used for the recall stage of recommendation systems in e-commerce, content platforms, and social networks.\n",
    "\n",
    "- Best For: Quickly building and experimenting with personalized recommendation systems. It includes tools for data preprocessing and model evaluation, making it a powerful choice for rapid development.\n",
    "\n",
    "3. **DeepCTR**: a Python library for **click-through rate (CTR) prediction** using deep learning models. CTR prediction is the task of estimating the probability that a user will click on an item.\n",
    "\n",
    "- Key Feature: It offers a wide range of pre-built deep learning-based models and tools to build recommendation systems, ad click prediction models, and other CTR-related tasks.\n",
    "\n",
    "- Best For: Building the ranking stage of a recommendation system, where you need a sophisticated model to predict which items a user is most likely to click on from a candidate set, and perform model evaluation and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef287e9-e141-4bd1-9311-f2701af8141d",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7976f2c-39a5-4c19-ae78-8aa2282b1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for Data Processing and Analysis\n",
    "import pandas as pd  # For data processing and analysis, providing powerful data structures and tools.\n",
    "import numpy as np  # For scientific computing, supporting multi-dimensional arrays and matrix operations.\n",
    "\n",
    "# Libraries for Data Structures and Mathematical Operations\n",
    "from collections import defaultdict, Counter  # Creates dictionaries with default values, automatically assigning them to non-existent keys.\n",
    "import collections  # Provides additional data structures like counters and ordered dictionaries.\n",
    "import math  # Provides mathematical functions, including trigonometric and logarithmic operations.\n",
    "\n",
    "# Libraries for Date and Time Handling\n",
    "from datetime import datetime  # For handling dates and times, including creation, formatting, and time calculations.\n",
    "import time  # Import the time module for handling time-related operations.\n",
    "\n",
    "# Miscellaneous Libraries \n",
    "import gc  # Import the garbage collection module for releasing memory space.\n",
    "from operator import itemgetter  # Import the itemgetter function from the operator module for retrieving elements based on an index or key.\n",
    "import os  # For interacting with the operating system, such as file operations and path handling.\n",
    "from tqdm import tqdm  # For displaying progress bars, useful for tracking progress when handling large datasets.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Controls the display of warnings, ignoring all warnings here.\n",
    "import pickle  # For serializing and deserializing Python objects, allowing saving to or loading from files.\n",
    "\n",
    "# Libraries for Similarity Search and Clustering\n",
    "import faiss  # For efficient similarity search and clustering algorithms.\n",
    "\n",
    "# Libraries for Random Number Generation and Feature Scaling\n",
    "import random  # Generates pseudo-random numbers.\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales features to a specific range.\n",
    "from sklearn.preprocessing import LabelEncoder  # Import the LabelEncoder class for label encoding.\n",
    "\n",
    "# Libraries for deepctr, tensorflow, deepmatch\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat  # From the DeepCTR library, used to define sparse and variable-length sparse features.\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K  # From TensorFlow's Keras, provides low-level operations for building and training deep learning models.\n",
    "from tensorflow.python.keras.models import Model  # For building neural network models, defining their structure and training process.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding, ensuring sequences have uniform length.\n",
    "from deepmatch.models import *  # Imports all models from the DeepMatch library, used for recommendation systems and related tasks.\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler  # From the DeepMatch library, a custom loss function for specific deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e1828-3aa2-421f-8ead-05ba5ae127ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, use this cell to load data\n",
    "from google.colab import drive\n",
    "\n",
    "# Connect to Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define file paths\n",
    "data_path = '/content/drive/MyDrive/Datasets/news-rec-sys/'\n",
    "save_path = '/content/drive/MyDrive/Datasets/news-rec-sys/temp_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71492167-08d7-4be9-bbdf-414c4eb76313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a local machine\n",
    "data_path = './data/' \n",
    "save_path = './data/temp_results/' # save temperary result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5feb8e-5949-48ca-a726-42f2d301c492",
   "metadata": {},
   "source": [
    "In a recommendation system project, we can use different data loading modes to handle data efficiently and effectively. This approach saves time and resources while allowing for thorough testing and validation.\n",
    "\n",
    "1. **Debug Mode** (Fast Development):\n",
    "This mode is for rapidly building and testing a baseline model. Because recommendation datasets are often massive, trying to work with the full data from the start is inefficient. Instead, we use a small, random sample of the training data (train_click_log_sample). This allows us to quickly confirm that our code is working correctly before we scale up to larger datasets.\n",
    "\n",
    "2. **Offline Validation Mode** (Model Selection & Tuning):\n",
    "This is where we evaluate and fine-tune our models before putting them into production. We load the full training dataset (train_click_log) and then split it into a smaller training set and a validation set. The training set is used to train the model, while the validation set helps us test different model architectures and adjust hyperparameters to find the best configuration.\n",
    "\n",
    "3. **Online Mode** (Final Prediction):\n",
    "This final mode is for making predictions on the unseen test data. After developing a working baseline and selecting an optimal model, we combine all available data (both train_click_log and test_click_log) to create a comprehensive \"full dataset.\" The model is then trained on this complete dataset to make the final predictions for the test set.\n",
    "\n",
    "These three modes provide a structured workflow that is both practical and efficient for developing high-performance recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ba9c9-deb4-4a3f-9e08-5e22ca11c8df",
   "metadata": {},
   "source": [
    "## 2. Functions for loading and preprocessing training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a607a9-f0d6-4803-abe1-1cdb05a057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard function for memory optimization\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()  # Record the start time of the function\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  # List of numeric data types\n",
    "    start_mem = df.memory_usage().sum / 1024**2  # Calculate the memory usage of the DataFrame (in Mb)\n",
    "\n",
    "    # Iterate through each column of the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_types = df[col].dtypes  # Get the data type of the column\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()  # Get the minimum value in the column\n",
    "            c_max = df[col].max()  # Get the maximum value in the column\n",
    "\n",
    "            # Check if there are missing values in the minimum and maximum values\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "\n",
    "            # Choose the appropriate data type conversion based on the data type's range\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum / 1024**2  # Calculate the memory usage of the DataFrame after conversion (in Mb)\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction), time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                  100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                  (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdaeb2f-2e13-451b-846f-c2a467fcf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode: Sample a portion of data from the training set for code debugging\n",
    "def get_all_click_sample(data_path, sample_nums=20000):\n",
    "    \"\"\"\n",
    "    Sample a portion of the training data for debugging\n",
    "    data_path: Path where the original data is stored\n",
    "    sample_nums: Number of samples to extract (sample a smaller number of users due to memory limitations)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(data_path, 'train_click_log.csv')  # use os.path.join to construct file path\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}!\")\n",
    "        \n",
    "    all_click = pd.read_csv(file_path)\n",
    "    all_user_ids = all_click['user_id'].unique()  # Get unique identifiers for all users\n",
    "    \n",
    "    # Randomly select a specified number of users from all users as sampled users\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False)\n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]  # Retain click data for sampled users\n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # Remove duplicates\n",
    "    return all_click\n",
    "\n",
    "\n",
    "# Load all click data, divided into online and offline modes.\n",
    "# If the goal is to validate model or feature effectiveness offline, we can use the training set only.\n",
    "# If the goal is to train an online inference model, the test dataset should be merged with the full dataset.\n",
    "def get_all_click_df(data_path, offline=True):\n",
    "    if offline:\n",
    "        file_path = os.path.join(data_path, 'train_click_log.csv') # use os.path.join to construct file path\n",
    "        if not file_path:\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}!\")\n",
    "        all_click = pd.read_csv(file_path)\n",
    "    else:\n",
    "        trn_click_path = os.path.join(data_path, 'train_click_log.csv') # use os.path.join to construct file path\n",
    "        tst_click_path = os.path.join(data_path, 'testA_click_log.csv') # use os.path.join to construct file path\n",
    "\n",
    "        if not os.path.exists(trn_click_path):\n",
    "            raise FileNotFoundError(f\"File not found: {trn_click_path}!\")\n",
    "        if not os.path.exists(tst_click_path):\n",
    "            raise FileNotFoundError(f\"File not found: {tst_click_path}!\")\n",
    "            \n",
    "        trn_click = pd.read_csv(trn_click_path)\n",
    "        tst_click = pd.read_csv(tst_click_path)\n",
    "        all_click = pd.concat([trn_click, tst_click])\n",
    "        \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # Remove duplicates\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b558de-44f5-4f7f-84e2-2c4f9db802b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic attributes of articles\n",
    "def get_item_info_df(data_path):\n",
    "    file_path = os.path.join(data_path, 'articles.csv') # use os.path.join to construct file path\n",
    "    if not file_path:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}!\")\n",
    "        \n",
    "    item_info_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Rename article_id to click_article_id to merge with click_article_id in the training set\n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362c2e6b-8be3-4f0a-9f75-b101ba2e265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article embedding data\n",
    "# Read the CSV file containing article embeddings and convert them into an embedding dictionary\n",
    "def get_item_emb_dict(data_path):\n",
    "    file_path = os.path.join(data_path, 'articles_emb.csv') # use os.path.join to construct file path\n",
    "    if not file_path:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}!\")\n",
    "        \n",
    "    item_emb_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Select columns containing 'emb' to retrieve embedding vectors\n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    # Convert the embedding columns into a NumPy array stored in contiguous memory\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "    # Normalize the embedding vectors to have a norm of 1 for comparability\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    # Create a dictionary mapping article IDs to their corresponding normalized embeddings\n",
    "    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "    # Save the embedding dictionary to a file using pickle\n",
    "    pickle.dump(item_emb_dict, open(save_path + 'item_content_emb.pkl', 'wb'))\n",
    "\n",
    "    return item_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7d62b8-4e96-4dcf-b6b4-2df26b002f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Samples\n",
    "#all_click_df = get_all_click_sample(data_path, sample_nums=2000)\n",
    "\n",
    "# Load the full training set\n",
    "all_click_df = get_all_click_df(data_path, offline=True)\n",
    "\n",
    "# Normalize the timestamps to calculate weights for association rules.\n",
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7af2851-a7a5-4314-afe5-9d334490828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article info\n",
    "item_info_df = get_item_info_df(data_path)\n",
    "\n",
    "# Load embedding dictionary\n",
    "item_emb_dict = get_item_emb_dict(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4e6d1-2d20-48b8-8ae0-80bfb7adb22e",
   "metadata": {},
   "source": [
    "## 3. Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b11ee0-0a61-41d7-834c-976adc9a3579",
   "metadata": {},
   "source": [
    "### 3.1 Get user-article-time dictionary\n",
    "This function will be used in user-based collaborative filtering with association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed378546-d6c9-4c0c-8402-aad16edb7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dictionary of users (key) and sequence of clicked by users based on click timestamps (value).   \n",
    "# {user1: [(item1, time1), (item2, time2)..]...}\n",
    "def get_user_item_time(click_df):\n",
    "    \"\"\"\n",
    "    Create a dictionary where the key is the User ID, and the value is a list of ariticle id along with the timestamps clicked by the user.\n",
    "    :param click_df: DataFrame containing user click information\n",
    "    :return: Dictionary mapping User IDs to lists of article-click timestamp pairs\n",
    "    \"\"\"\n",
    "    def make_item_time_pair(df):\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "        \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']]\\\n",
    "                                .apply(lambda x: make_item_time_pair(x))\\\n",
    "                                .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "\n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1104a-b4fd-45b1-92c1-541a7e831c7c",
   "metadata": {},
   "source": [
    "### 3.2 Get article-user-time dictionary\n",
    "This function will be used in item-based collaborative filtering with association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76cb9380-8820-4728-ba93-5f29ae6e101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dictionary of items (key) and sequence of users based on click timestamps (value).   \n",
    "# {item1: [(user1, time1), (user2, time2)...]...}\n",
    "def get_item_user_time_dict(click_df):\n",
    "    \"\"\"\n",
    "    Create a dictionary where the key is the item ID, and the value is a list of users who clicked the item along with the timestamps.\n",
    "    :param click_df: DataFrame containing user click information\n",
    "    :return: Dictionary mapping item IDs to lists of user-click timestamp pairs\n",
    "    \"\"\"\n",
    "    def make_user_time_pair(df):\n",
    "        return list(zip(df['user_id'], df['click_timestamp']))\n",
    "        \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    item_user_time_df = click_df.groupby('click_article_id')[['user_id', 'click_timestamp']]\\\n",
    "                                .apply(lambda x: make_user_time_pair(x))\\\n",
    "                                .reset_index().rename(columns={0: 'user_time_list'})\n",
    "    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "    \n",
    "    return item_user_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36575e-c57c-42c0-b058-58ab91eda8bf",
   "metadata": {},
   "source": [
    "### 3.3 Get historical and last clicks dataframes \n",
    "This will be used in evaluating recall results, feature engineering, and creating labels for converting into a supervised learning test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f827fb42-74c2-4627-9d48-770848af9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical and last clicks from the current data\n",
    "def get_hist_and_last_click(click_df):\n",
    "    click_df = click_df.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = click_df.groupby('user_id').tail(1)\n",
    "\n",
    "    # If the user has only one click record (len(user_df) == 1),\n",
    "    # the hist_func will return the entire click record for that user.\n",
    "    # Otherwise, it will return all clicks except the last one.\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = click_df.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7bf6e-758a-4eb4-95a5-efeae05d18f4",
   "metadata": {},
   "source": [
    "### 3.4 Get Article Attribute Features (dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daac285a-6671-4faf-aef5-41f813b0589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Retrieve the basic attributes corresponding to article IDs and save them as a dictionary for easy use during the recall and cold start phases.\n",
    "def get_item_info_dict(item_info_df):\n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)\n",
    "\n",
    "    item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "\n",
    "    return item_type_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4b2bd-db7f-4ca3-82be-e061a9494ab1",
   "metadata": {},
   "source": [
    "### 3.5 Get Information of Articles Clicked in User's History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad61074e-2882-480c-b9d2-38e33f627933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_hist_item_info_dict(click_df):\n",
    "    \"\"\"\n",
    "    This function retrieves various information from the user's historical clicks and returns the following dictionaries:\n",
    "    1. A dictionary of the set of article categories clicked by the user.\n",
    "    2. A dictionary of the set of article IDs clicked by the user.\n",
    "    3. A dictionary of the average word count of articles clicked by the user.\n",
    "    4. A dictionary of the normalized creation time of the last article clicked by the user.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve a dictionary of the set of article categories clicked by each user (user_id).\n",
    "    user_hist_item_types = click_df.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "    user_hist_item_types_dict = dict(zip(user_hist_item_types['user_id'], user_hist_item_types['category_id']))\n",
    "\n",
    "    # Retrieve a dictionary of the set of article IDs clicked by each user (user_id).\n",
    "    user_hist_item_ids_dict = click_df.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "\n",
    "    # Retrieve a dictionary of the average word count of articles clicked by each user (user_id).\n",
    "    user_hist_item_words = click_df.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "\n",
    "    # Retrieve the creation time of the last article clicked by each user (user_id).\n",
    "    click_df_ = click_df.sort_values('click_timestamp')\n",
    "    user_last_item_created_time = click_df_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "\n",
    "    # Normalize created_at_ts\n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n",
    "\n",
    "    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "\n",
    "    return user_hist_item_types_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9626567-f51c-45e4-b513-a054dc5b42f3",
   "metadata": {},
   "source": [
    "The importance of these four dictionaries:\n",
    "- **User Interest Categories**: By collecting and analyzing the categories of articles a user clicks on, we can build a profile of their interests. This set of categories is a powerful tool for making more personalized and accurate recommendations.\n",
    "\n",
    "- **User Click History**: Keeping a record of every article a user has clicked on allows us to understand their historical behavior. This click sequence is essential for building sophisticated sequential models, which can predict future interests based on past actions.\n",
    "\n",
    "- **Reading Habits**: Calculating the average word count of the articles a user reads helps us understand their reading preferences. This feature can be used to recommend articles that are a suitable length, improving the user's experience.\n",
    "\n",
    "- **Recency and Timeliness**: Tracking the creation time of the most recent article a user clicked on is crucial. The latest clicks are often the most accurate reflection of a user's current interests, so we can use this information to prioritize fresh recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6e598-aa6a-4533-a30d-9c4614a7eddf",
   "metadata": {},
   "source": [
    "### 3.6 Retrieve the Top-K Most Clicked Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c7056b-c81b-4b4d-8ca4-61be9e1ce711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Most Clicked Recent Articles\n",
    "def get_item_topk_click(click_df, k):\n",
    "    topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "    return topk_click"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df242e57-75ad-4eb8-b256-65b10c30687f",
   "metadata": {},
   "source": [
    "## 4. Define a multi-channel recall dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa47b8c-07d4-40b7-8174-0d05132af6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Article Attribute Information and Store as a Dictionary for Easy Lookup\n",
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)\n",
    "\n",
    "# Extract user history and last clicks\n",
    "trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ac6349-e79f-4244-91b9-103ce7b1a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Multi-Channel Recall Dictionary to Store Results from Different Recall Strategies\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'youtubednn_usercf_recall': {},\n",
    "                           'cold_start_recall': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb8828-b957-4bbf-a780-1e9a686622f4",
   "metadata": {},
   "source": [
    "- `'itemcf_sim_itemcf_recall': {}`: This dictionary is for storing results from item-based collaborative filtering. It will hold the candidate articles retrieved by this algorithm, which finds items similar to a user's past clicks.\n",
    "- `'embedding_sim_item_recall': {}`: This dictionary is for results from an embedding similarity algorithm. It will store the candidate articles found by comparing the vector embeddings of items to find those that are similar.\n",
    "- `'cold_start_recall': {}`: This is a dedicated dictionary for handling the cold-start problem. It will contain the recall results for new users who have little to no historical data.\n",
    "-  `'youtubednn_recall': {}`: This dictionary is for storing results from a YouTube DNN (Deep Neural Network) model. This model learns to represent users and items as vectors (embeddings) and then uses a fast similarity search to find the most relevant items for a given user. This dictionary will hold the candidate articles retrieved by this model.\n",
    "-  `'youtubednn_usercf_recall': {}`: This is a more specific and advanced recall strategy that combines the power of the YouTube DNN with user-based collaborative filtering. This approach first uses a DNN model to find a user's embedding, then find other users with similar embeddings, and finally recommend items that those similar users have liked. This dictionary will hold the candidate articles generated through this hybrid method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505491e-eb23-4e36-bf32-809bf5db4a9c",
   "metadata": {},
   "source": [
    "## 5. Recall Evaluation Function  \n",
    "We need to evaluate our recall process and adjust the current recall method or parameters to achieve better recall performance. Since the recall results determine the upper limit of the final ranking, the following function provides a method for recall evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a947c95d-bf73-4c20-bab1-65ed4eb6a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=50):\n",
    "    \"\"\"\n",
    "    This function evaluates the hit rate for the top 10, 20, 30, 40, and 50 recalled articles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the user's last click records into a dictionary with user_id as the key and click_article_id as the value\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "\n",
    "    # Get the number of users in the recall dictionary\n",
    "    user_num = len(user_recall_items_dict)\n",
    "\n",
    "    # Evaluate the hit rate for the top 10, 20, 30, 40, and 50 recalled articles\n",
    "    for k in range(10, topk + 1, 10):\n",
    "        hit_num = 0  # Record the number of hits\n",
    "\n",
    "        # Iterate through each user and their list of recalled articles\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            # Get the top-k recall results (article IDs)\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "\n",
    "            # If the user's last clicked article is in the top-k recall results, count it as a hit\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1\n",
    "\n",
    "        # Calculate the hit rate: hit rate = number of hits / total number of users\n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "\n",
    "        # Output the result\n",
    "        print('topk:', k, ' | hit_num:', hit_num, ' | hit_rate:', hit_rate, ' | user_num:', user_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417f856-ad64-4d0a-8977-d4b3ccb1a08e",
   "metadata": {},
   "source": [
    "## 6. Calculate the Similarity Matrix  \n",
    "This section focuses on obtaining the similarity matrix through collaborative filtering and vector search. The similarity matrix is divided into `user2user` and `item2item`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db449653-1cda-4531-981b-235f1fef5847",
   "metadata": {},
   "source": [
    "### 6.1 Item Collaborative Filtering - item2item similarity \n",
    "We first obtain the `item2item` similarity matrix based on item-based collaborative filtering (itemCF). When calculating the item-to-item similarity matrix using association rules, the similarity of articles also takes into account:\n",
    "\n",
    "- **Time weight of user clicks**: The recency of a user's click.\n",
    "\n",
    "- **Sequence weight of user clicks**: The order in which a user clicked on articles.\n",
    "\n",
    "- **Creation time weight of articles**: The recency of the article's publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89bc9f40-207e-4ed1-a7e8-69fe3aafede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(click_df, item_created_time_dict, option='raw'):\n",
    "    \"\"\"\n",
    "    Item similarity matrix calculation.\n",
    "    :param df: DataFrame\n",
    "    :item_created_time_dict: Dictionary of article creation times\n",
    "    :return: The article-to-article similarity matrix\n",
    "\n",
    "    Approach: Item-based Collaborative Filtering + Association Rules\n",
    "    \"\"\"\n",
    "    # Get a dictionary of items which clicked by users\n",
    "    # Key is the user ID, value is the list of items that the user clicked.\n",
    "    user_item_time_dict = get_user_item_time(click_df)\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    i2i_sim = defaultdict(dict)\n",
    "    item_cnt = defaultdict(int)\n",
    "\n",
    "    if option == 'raw':\n",
    "        print('Raw similarity calculation\\n')\n",
    "        # Raw similarity accumulation process\n",
    "        # Iterate over each user and their list of clicked items and times\n",
    "        for user, item_time_list in tqdm(user_item_time_dict.items(),\n",
    "                                         desc=\"Building item-item similarity matrix (raw)\"):\n",
    "            # Iterate over each item i clicked by the user\n",
    "            for i, i_click_time in item_time_list:\n",
    "                item_cnt[i] += 1  # Record the number of times item i was clicked\n",
    "\n",
    "                # Nested iteration over each pair of items i and j clicked by the user\n",
    "                for j, j_click_time in item_time_list:\n",
    "                    if i == j:  # Skip if item i and item j are the same\n",
    "                        continue\n",
    "\n",
    "                    # Initialize similarity between i and j to 0 if not already set\n",
    "                    i2i_sim[i].setdefault(j, 0)                   \n",
    "                    # Accumulate similarity between item i and item j\n",
    "                    i2i_sim[i][j] += 1 / math.log(len(item_time_list) + 1)\n",
    "                    \n",
    "    elif option == 'weighted':\n",
    "        print('Weighted similarity calculation\\n')\n",
    "        for user, item_time_list in tqdm(user_item_time_dict.items(),\n",
    "                                         desc=\"Building item-item similarity matrix (weighted)\"):\n",
    "            # Iterate over each item i clicked by the user\n",
    "            for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "                item_cnt[i] += 1  # Record the number of times item i was clicked\n",
    "                i2i_sim.setdefault(i, {})\n",
    "                \n",
    "                # Nested iteration over each pair of items i and j clicked by the user\n",
    "                for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                    if i == j:  # Skip if item i and item j are the same\n",
    "                        continue\n",
    "\n",
    "                    # Consider both forward and backward sequential clicks\n",
    "                    loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                    # Positional weight; parameters can be adjusted\n",
    "                    loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                    # Click time weight; parameters can be adjusted\n",
    "                    click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                    # Article creation time weight; parameters can be adjusted\n",
    "                    created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                    # Initialize similarity between i and j to 0 if not already set\n",
    "                    i2i_sim[i].setdefault(j, 0)                   \n",
    "                    # Accumulate similarity between item i and item j considering multiple factors\n",
    "                    i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "    \n",
    "    # Normalize the similarity values  \n",
    "    i2i_sim_final = defaultdict(dict)\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_final[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "\n",
    "    # Save the similarity matrix to local path\n",
    "    with open(save_path + 'itemcf_i2i_sim.pkl', 'wb') as f:\n",
    "        pickle.dump(i2i_sim_final, f)\n",
    "\n",
    "    return i2i_sim_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34c07e-4d5f-46ab-81cd-ea1d46c5abba",
   "metadata": {},
   "source": [
    "#### About Similarity Calculation\n",
    "\n",
    "1. **Similarity Accumulation Process**:\n",
    "\n",
    "   $$\n",
    "   \\text{sim}(i, j) = \\sum_{u \\in U_{ij}} \\frac{1}{\\log(1 + |N(u)|)}\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ \\text{sim}(i, j) $ represents the similarity between item $ i $ and item $ j $.\n",
    "   - $ U_{ij} $ denotes the set of users who clicked both item $ i $ and item $ j $.\n",
    "   - $ N(u) $ refers to the number of items clicked by user $ u $.\n",
    "\n",
    "2. **Similarity Normalization Process**:\n",
    "\n",
    "   $$\n",
    "   \\text{sim\\_normalized}(i, j) = \\frac{\\text{sim}(i, j)}{\\sqrt{c(i) \\times c(j)}}\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ \\text{sim\\_normalized}(i, j) $ denotes the normalized similarity between item $ i $ and item $ j $.\n",
    "   - $ c(i) $ and $ c(j) $ represent the total number of clicks for item $ i $ and item $ j $, respectively.\n",
    "\n",
    "Similarity is calculated based on co-occurrence frequency. `1 / log(len(item_time_list) + 1)` or `weight / log(len(item_time_list) + 1)` is used as the increment value for similarity to standardize across different lengths of user click lists.\n",
    "\n",
    "- **Co-occurrence Frequency**: The similarity value reflects how often two items appear together in the same userâ€™s click history. The more frequent the co-occurrence, the higher the similarity.\n",
    "- **Normalization**: `1 / math.log(len(item_time_list) + 1)` normalizes the similarity increment to reduce the influence of long click lists on the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26230173-50a1-41fc-af58-b83499ec1410",
   "metadata": {},
   "source": [
    "The weighted similarity considers:\n",
    "- **Positional Weight** `loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))`: This weight accounts for the order and distance of a user's clicks. `loc_alpha = 1.0 if loc2 > loc1 else 0.7` is a directional modifier. It assigns a higher value (1.0) if a user clicks article j after article i, and a lower value (0.7) if the order is reversed. This gives more importance to the most recent behavior. The positional weight decays exponentially based on the distance between the clicks `np.abs(loc2 - loc1)`. The further apart the two clicks are in a user's history, the smaller this weight becomes. The 0.9 is a decay coefficient that controls how quickly this weight diminishes.\n",
    "- **Click Time Weight** `click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))`: This weight is based on how much time passed between a user's clicks on articles i and j. The closer the clicks are in time, the higher the weight. This assumes that a user's interests are more similar for items they engage with in quick succession. The 0.7 is a decay coefficient that controls the speed of this decay.\n",
    "- **Article Creation Time Weight** `created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))`: This weight considers the freshness of the articles themselves. It gives a higher weight to articles that were created around the same time. This can capture trends where users are interested in a specific topic that is currently being published. The 0.8 is the decay coefficient.\n",
    "- **Final Similarity Score** `i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)`: The final similarity score `i2i_sim[i][j]` is a composite of the above factors. It's calculated by multiplying the three weights together (loc_weight * click_time_weight * created_time_weight) and then dividing by a logarithmic term math.log(len(item_time_list) + 1). This final division is a normalization step to prevent a single user's frequent clicks from disproportionately inflating the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2ead000-3a5e-4c62-9488-6a32deb5a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted similarity calculation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building item-item similarity matrix (weighted): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [02:27<00:00, 1358.32it/s]\n"
     ]
    }
   ],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict, option='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21f197-8a12-4942-a45a-e6fd2d730d5f",
   "metadata": {},
   "source": [
    "### 6.2 User Collaborative Filtering - user2user similarity \n",
    "We then obtain the `user2user` similarity matrix based on user-based collaborative filtering (userCF). When calculating the user-to-user similarity matrix, some simple association rules can also be applied, such as **user activity weight**. Here, the number of user clicks is used as an indicator of user activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "830e1e3b-62c4-4cee-a8ee-80119bf7aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_active_degree_dict(click_df):\n",
    "    click_df_ = click_df.groupby('user_id')['click_article_id'].count().reset_index()\n",
    "\n",
    "    # Normalize user activity\n",
    "    mm = MinMaxScaler()\n",
    "    click_df_['click_article_id'] = mm.fit_transform(click_df_[['click_article_id']])\n",
    "    user_active_degree_dict = dict(zip(click_df_['user_id'], click_df_['click_article_id']))\n",
    "\n",
    "    return user_active_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c089fe89-223e-4d50-b295-e8b2e5488fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usercf_sim(click_df, user_active_degree_dict):\n",
    "    \"\"\"\n",
    "    User similarity matrix calculation.\n",
    "    :param all_click_df: DataFrame\n",
    "    :param user_activate_degree_dict: Dictionary of user activity levels\n",
    "    :return: User similarity matrix\n",
    "    \"\"\"\n",
    "    # Get a dictionary of users who clicked on each item.\n",
    "    # Key is the item ID, value is the list of users who clicked the item.\n",
    "    item_user_time_dict = get_item_user_time_dict(click_df)\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    u2u_sim = defaultdict(dict)\n",
    "    user_cnt = defaultdict(int)\n",
    "\n",
    "    # Iterate over each item and its list of users who clicked it\n",
    "    for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "        # Iterate over each user and their click time\n",
    "        for u, u_click_time in user_time_list:\n",
    "            # Count the number of items clicked by each user\n",
    "            user_cnt[u] += 1\n",
    "            # Nested iteration over users to calculate user similarity\n",
    "            for v, v_click_time in user_time_list:\n",
    "                # Skip if the two users are the same (no self-similarity calculation)\n",
    "                if u == v:\n",
    "                    continue\n",
    "                # Initialize the similarity value for users if not already set\n",
    "                u2u_sim[u].setdefault(v, 0)\n",
    "                # Calculate the similarity weight considering user activity\n",
    "                active_weight = 100 * 0.5 * (user_active_degree_dict[u] + user_active_degree_dict[v])\n",
    "                # Update the similarity value in the matrix for the two users\n",
    "                u2u_sim[u][v] += active_weight / math.log(len(user_time_list) + 1)\n",
    "\n",
    "    # Copy the user similarity matrix to avoid modifying the original data \n",
    "    u2u_sim_ = u2u_sim.copy()\n",
    "    # Normalize the similarity values in the user similarity matrix\n",
    "    for u, related_users in u2u_sim.items():\n",
    "        for v, wij in related_users.items():\n",
    "            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])\n",
    "                \n",
    "    # Save the user similarity matrix to the local storage\n",
    "    pickle.dump(u2u_sim_, open(save_path + 'usercf_u2u_sim.pkl', 'wb'))\n",
    "\n",
    "    return u2u_sim_       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b87a149-f112-46f6-9f7b-3cad6c0fff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1735/1735 [00:10<00:00, 159.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculating UserCF is too memory-intensive, so we use a sampled dataset to run.\n",
    "user_active_degree_dict = get_user_active_degree_dict(all_click_df)\n",
    "u2u_sim = usercf_sim(all_click_df[:20000], user_active_degree_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f54b4b-d75e-44cf-995b-71e23163ba2e",
   "metadata": {},
   "source": [
    "### 6.3 Item Embedding Similarity\n",
    "Item embedding similarity is a method for finding articles that are semantically or contextually similar to each other. Every article is represented by a unique vector (an \"embedding\") in a high-dimensional space. The closer two vectors are in this space, the more similar the articles they represent.\n",
    "\n",
    "This is particularly useful for new articles that have no click data (a cold-start scenario). By comparing a new article's embedding to the embeddings of all other articles in the database, we can find a set of similar articles that can be recommended to users who have previously shown an interest in that topic.\n",
    "\n",
    "#### An Introduction to Faiss\n",
    "Faiss is a core component in modern recommendation systems. Developed by Facebook's AI team, it is a high-performance library built for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "In a large-scale recommendation system, simply calculating the similarity between every item and every other item taking a prohibitive amount of time during online inference. Faiss solves this problem by using advanced algorithms that can perform an approximate nearest neighbor search (ANN).\n",
    "\n",
    "This means that instead of finding the perfect nearest neighbor, Faiss finds a vector that is a very close approximation, and it does so in a fraction of the time. This trade-off between absolute accuracy and speed is essential for real-world applications where a massive search must be completed quickly.\n",
    "\n",
    "**The key benefit of Faiss is its ability to find the top-k most similar vectors to a given query vector at an incredible speed**, making it the perfect tool for the vector recall stage of a recommendation system.\n",
    "\n",
    "#### **Principle of Faiss Query:**\n",
    "\n",
    "1. **Index Structure Construction:**  \n",
    "   In Faiss, the first step is to build an index structure for the vector dataset to be searched. Faiss provides various index structures, such as flat indexes, inverted file indexes (IVF), and product quantization indexes (PQ). Each index structure is suited to specific scenarios and requirements, allowing users to choose the most appropriate one based on their needs.\n",
    "\n",
    "2. **Vector Encoding:**  \n",
    "   Before constructing the index structure, vector data typically needs to be encoded. The purpose of encoding is to map high-dimensional vectors into a lower-dimensional space to reduce computation and storage costs. Common encoding methods include vector quantization and hash encoding.\n",
    "\n",
    "3. **Query Process:**  \n",
    "   Once the index structure is built, queries can be performed on it. The query process involves calculating the similarity between the query vector and the vectors stored in the index structure. Faiss provides multiple query methods, such as exact search and approximate search. In approximate search, Faiss uses algorithms such as Product Quantization (PQ) and Locality Sensitive Hashing (LSH) to accelerate the search process.\n",
    "\n",
    "4. **Result Retrieval:**  \n",
    "   After the query is complete, Faiss returns the indexes or distances of the top-k most similar vectors. Users can specify the number of results they need and process or analyze the returned results further.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Product Quantization (PQ):**  \n",
    "   - PQ divides a high-dimensional vector into multiple sub-vectors and independently quantizes each sub-vector. This maps the original high-dimensional vector into a lower-dimensional subspace, reducing computation and storage requirements.\n",
    "   - The key to PQ lies in how the original vector is divided into sub-vectors and how appropriate quantization methods are designed. Common methods include Product Quantization and Residual Product Quantization.\n",
    "   - PQ significantly reduces search time while maintaining search quality, making it especially useful for large-scale high-dimensional data in approximate nearest neighbor searches.\n",
    "\n",
    "2. **Locality Sensitive Hashing (LSH):**  \n",
    "   - LSH is a hash-based method for approximate nearest neighbor search. Its core idea is to map similar vectors into the same bucket so that similar vectors can be quickly located during the query.\n",
    "   - LSH uses multiple hash functions to generate several hash values for each vector, which are then used to map the vectors into buckets. During a query, only the vectors in the query vectorâ€™s bucket and neighboring buckets are searched, eliminating the need to traverse the entire dataset.\n",
    "   - LSH provides high search efficiency and scalability when dealing with large-scale data, particularly for high-dimensional data in approximate nearest neighbor searches.\n",
    "\n",
    "---\n",
    "\n",
    "Faiss utilizes two key techniques for vector compression and encoding: **PCA** (Principal Component Analysis) and **PQ** (Product Quantization), along with other optimization techniques. However, PCA and PQ are the core components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40c0ca2a-eb08-4d20-b33d-a9e50bc3e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Retrieval Similarity Calculation\n",
    "# topk refers to the number of most similar items returned by Faiss after searching for each individual item.\n",
    "def embedding_sim(item_emb_df, save_path, topk):\n",
    "    \"\"\"\n",
    "    Content-based article embedding similarity matrix calculation.\n",
    "    :param click_df: DataFrame containing click data.\n",
    "    :param item_emb_df: DataFrame containing article embeddings.\n",
    "    :param save_path: Path to save the similarity matrix.\n",
    "    :param topk: Number of most similar articles to retrieve.\n",
    "    :return: Article similarity matrix.\n",
    "\n",
    "    Approach: For each article, return the top-k most similar articles based on embedding similarity.\n",
    "    Since the number of articles is large, Faiss is used to accelerate the process.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping between article index and article ID\n",
    "    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df['article_id']))\n",
    "\n",
    "    # Extract embedding columns\n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n",
    "\n",
    "    # Normalize the vectors\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    # Build Faiss index\n",
    "    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])  # Inner Product index, embedding vector dimension as parameter\n",
    "    item_index.add(item_emb_np)  # Add embeddings to the index\n",
    "\n",
    "    # Perform similarity search for each article, returning top-k similar vectors\n",
    "    sim, idx = item_index.search(item_emb_np, topk)\n",
    "\n",
    "    # Dictionary to store similarity results with original article IDs\n",
    "    item_sim_dict = defaultdict(dict)\n",
    "\n",
    "    # Add progress bar using tqdm\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n",
    "        target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "        # Start from 1 to exclude the article itself, so the final result contains topk-1 similar articles\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "\n",
    "    # Save the item-to-item similarity matrix as a pickle file\n",
    "    pickle.dump(item_sim_dict, open(save_path + 'emb_i2i_sim.pkl', 'wb'))\n",
    "\n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218f1767-8604-4a34-960f-99430b8b9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364047it [00:12, 28462.65it/s]\n"
     ]
    }
   ],
   "source": [
    "item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "emb_i2i_sim = embedding_sim(item_emb_df, save_path, topk=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc616972-95ec-492e-a2a3-c387cf16255a",
   "metadata": {},
   "source": [
    "## 7. The Recall Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a503a-76be-4fb9-a751-119612e900a0",
   "metadata": {},
   "source": [
    "The recall phase is a crucial first step in a recommendation system. Its job is to quickly narrow down a vast number of items to a smaller, more manageable set of potential candidates that a user might be interested in.\n",
    "\n",
    "#### How Recall Works\n",
    "Imagine you're on a shopping website. The system quickly looks at your browsing history, past purchases, and other data to find a large pool of products you might like. This could mean pulling up all electronics because you recently bought a new laptop, or all items in a specific category you frequently browse.\n",
    "\n",
    "The goal isn't to find the perfect item yet, but to efficiently find a wide array of possibilities. These candidates are then passed on to the next step, the ranking phase, which will score and sort them to find the best few to show you.\n",
    "\n",
    "This two-step approach saves a lot of computing power and makes the entire recommendation process much faster and more efficient.\n",
    "\n",
    "#### Strategies for Scaling Down the Problem\n",
    "The recall phase addresses the problem we discussed at the beginning: how do we reduce the scale of a problem involving 360,000 articles and over 200,000 users? We can use the recall phase to filter a set of candidate articles for each user, thus reducing the problem's scale. Common recall strategies include:\n",
    "\n",
    "- **YouTube DNN Recall**: A deep learning approach that uses a user's past behavior (like watch and click history) to predict which articles they are most likely to click on. This is a very effective way to narrow down a huge list of candidates.\n",
    "\n",
    "- **Item-based Collaborative Filtering**: This method finds articles that are similar to the ones a user has already clicked on, and adds these similar articles to the candidate set.\n",
    "\n",
    "- **User-based Collaborative Filtering**: This method finds other users who have similar interests and behavior as the user. It then recommends articles that those similar users have engaged with.\n",
    "\n",
    "- **Content-based Filtering**: This approach uses article embedding vectors to measure the similarity between articles, and then performs recall based on this similarity. For example, models like Word2Vec or Doc2Vec can provide these embeddings.\n",
    "\n",
    "- **User Embedding**: Similar to content-based filtering, this method represents each user as a vector. It then finds other users with similar vectors and recommends their content. This is a powerful way to make personalized recommendations.\n",
    "\n",
    "Each of these methods provides a unique way to calculate similarity, whether it's based on user behavior, article content, or a combination of both. By combining these strategies, we can create a robust and efficient system that provides a strong foundation for the final recommendation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9be61-ca19-4098-b4ab-f8fe918e80c9",
   "metadata": {},
   "source": [
    "### 7.1 YoutubeDNN Recall\n",
    "Covington et al. 2016 (https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf)\n",
    "![](https://wuciawe.github.io/files/2019-05-15-notes-on-youtube-dnn/candidate_generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1385a0-5a29-4b5d-8f61-1f12ee2d70d2",
   "metadata": {},
   "source": [
    "YoutubeDNN uses a negative sampling technique to train the model. The below function is for generating training and test data. A small set of negative samples (articles not clicked by the user) will be chosen for each positive sample (articles clicked by the user). \n",
    "\n",
    "This function is designed to generate training and test sets from a dataset of user clicks. It takes the following parameters:\n",
    "- data: A DataFrame containing the raw click data.\n",
    "- negsample: The number of negative samples to be chosen from unclicked articles when building the training windows.\n",
    "\n",
    "The generating function works as follows:\n",
    "1. Sorts the data by click time to ensure correct chronological order.\n",
    "2. Gathers a list of all unique article IDs.\n",
    "3. Initializes empty lists to store the training and test sets.\n",
    "4. Groups the data by user_id to process each user individually.\n",
    "5. For each user, it gets their list of clicked articles to serve as positive samples.\n",
    "6. If negsample is greater than zero, it randomly selects negative samples from the pool of articles the user has not clicked.\n",
    "7. If a user has only clicked one article, that click is added to both the training and test sets.\n",
    "8. It uses a sliding window method to create positive and negative sample pairs. The last click in the sequence for each user is reserved for the test set.\n",
    "9. Randomly shuffles the order of the training and test sets.\n",
    "10. Finally, it returns the generated training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7817ee6-1488-4a94-82fe-d8fa6ef45618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training and Validation Data for Two-Tower Recall\n",
    "# negsample refers to the number of negative samples chosen when constructing the training samples using a sliding window\n",
    "def gen_data_set(data, negsample=0):\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    item_ids = data['click_article_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for userID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()  # positive sample list\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))  # Select negative samples from articles the user has not seen.\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True) # For each positive sample, select n negative samples.\n",
    "            \n",
    "        # When the sequence length is only one, this data point must also be included in the training set. Otherwise, the final learned embeddings will have missing values.\n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((userID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "            test_set.append((userID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "\n",
    "        # Constructing positive and negative samples using a sliding window.\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((userID, hist[::-1], pos_list[i], 1, len(hist[::-1])))  # Positive Sample [user_id, his_item, pos_item, label, len(his_item)]\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((userID, hist[::-1], neg_list[i*negsample+negi], 0, len(hist[::-1])))  # negative sample [user_id, his_item, neg_item, label, len(his_item)]\n",
    "                else:\n",
    "                    # Use the longest sequence length as the test data.\n",
    "                    test_set.append((userID, hist[::-1], pos_list[i], 1, len(hist[::-1])))\n",
    "    \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69487fd-2be0-48dd-9ece-4b8e9264d6de",
   "metadata": {},
   "source": [
    "The below function is for Padding and Preparing Model Input. This function is used to pad the input data so that all sequence features have a consistent length, which is a requirement for many deep learning models. It takes these parameters:\n",
    "\n",
    "- train_set: The training samples generated by the gen_data_set function.\n",
    "- user_profile: A DataFrame containing user attribute information.\n",
    "- seq_max_len: The maximum length for the sequence features.\n",
    "\n",
    "The function's main steps are:\n",
    "1. Extracts user IDs, historical article sequences, clicked article IDs, labels, and historical sequence lengths from the train_set.\n",
    "2. Pads the historical article sequences to ensure they all have the same length, defined by seq_max_len.\n",
    "3. Builds a dictionary of model inputs, including the user ID, clicked article ID, padded historical sequence, and the original historical sequence length.\n",
    "4. Returns the model input dictionary and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e10f6d63-6ffd-41ee-a327-2c53a0a1a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the input data to ensure that all sequence features have a uniform length\n",
    "def gen_model_input(train_set, seq_max_len):\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"click_article_id\": train_iid, \"hist_article_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fac31d7d-b3a6-4a68-bed3-1ddc33442ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtubednn_u2i_dict(data, topk=20):\n",
    "    SEQ_LEN = 30 # For the user click sequence length, pad the short sequences and truncate the long ones.\n",
    "\n",
    "    user_profile_ = data[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile_ = data[[\"click_article_id\"]].drop_duplicates('click_article_id')\n",
    "\n",
    "    # Label Encoding\n",
    "    features = [\"click_article_id\", \"user_id\"]\n",
    "    feature_max_idx = {}\n",
    "\n",
    "    for feature in features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feature] = lbe.fit_transform(data[feature])\n",
    "        feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "    # Extracting user and item profiles. The specific features to select here require further analysis and consideration.\n",
    "    user_profile = data[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile = data[[\"click_article_id\"]].drop_duplicates('click_article_id')\n",
    "\n",
    "    user_index_2_rawid = dict(zip(user_profile['user_id'], user_profile_['user_id']))\n",
    "    item_index_2_rawid = dict(zip(item_profile['click_article_id'], item_profile_['click_article_id']))\n",
    "\n",
    "    # Splitting into Training and Test Sets\n",
    "    # Because deep learning typically requires a very large amount of data, we often expand the training samples using a sliding window approach to ensure a high-quality recall performance.\n",
    "    train_set, test_set = gen_data_set(data, 1)\n",
    "\n",
    "    # Generate model input\n",
    "    train_model_input, train_label = gen_model_input(train_set, SEQ_LEN)\n",
    "    test_model_input, test_label = gen_model_input(test_set, SEQ_LEN)\n",
    "\n",
    "    # Dimension of embedding\n",
    "    embedding_dim = 16\n",
    "\n",
    "    # Organize the data into a format that the model can directly accept.\n",
    "    user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "                            VarLenSparseFeat(SparseFeat('hist_article_id', feature_max_idx['click_article_id'], embedding_dim,\n",
    "                                                        embedding_name=\"click_article_id\"), SEQ_LEN, 'mean', 'hist_len'),]\n",
    "    item_feature_columns = [SparseFeat('click_article_id', feature_max_idx['click_article_id'], embedding_dim)]\n",
    "\n",
    "    train_counter = Counter(train_model_input['click_article_id'])\n",
    "    item_count = [train_counter.get(i, 0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "    sampler_config = NegativeSampler('frequency', num_sampled=5, item_name=\"click_article_id\", item_count=item_count)\n",
    "\n",
    "    if tf.__version__ >= '2.0.0':\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "    else:\n",
    "        K.set_learning_phase(True)\n",
    "        \n",
    "    # Model Definition\n",
    "    # num_sampled: The number of samples to use during negative sampling.\n",
    "    model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(64, embedding_dim), sampler_config=sampler_config)\n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "\n",
    "    # Model Training\n",
    "    # The proportion of the validation can be defined by validation_split. If it is set to 0, the model will be trained directly on the full dataset.\n",
    "    history = model.fit(train_model_input, train_label, batch_size=1024, epochs=30, verbose=1, validation_split=0.2)\n",
    "\n",
    "    # After the model has finished training, extract the learned embeddings, including both the user and item embeddings.\n",
    "    test_user_model_input = test_model_input\n",
    "    all_item_model_input = {\"click_article_id\": item_profile['click_article_id'].values}\n",
    "\n",
    "    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "    # Save the current item and user embeddings. They might be useful for the ranking phase, but be sure to save them in a way that corresponds to their original IDs.\n",
    "    user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "    # Normalize the embeddings before saving.\n",
    "    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "    item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n",
    "\n",
    "    # Convert the embeddings into a dictionary format for easy querying.\n",
    "    raw_user_id_emb_dict = {user_index_2_rawid[k]: \\\n",
    "                                v for k, v in zip(user_profile['user_id'], user_embs)}\n",
    "    raw_item_id_emb_dict = {item_index_2_rawid[k]: \\\n",
    "                                v for k, v in zip(item_profile['click_article_id'], item_embs)}\n",
    "\n",
    "    # Save the embeddings locally.\n",
    "    pickle.dump(raw_user_id_emb_dict, open(save_path + 'user_youtube_emb.pkl', 'wb'))\n",
    "    pickle.dump(raw_item_id_emb_dict, open(save_path + 'item_youtube_emb.pkl', 'wb'))\n",
    "\n",
    "    # Faiss nearest neighbor search: use user embeddings to search for the top-k most similar items\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    # Normalization has already been performed above, so we can skip it here\n",
    "    #faiss.normalize_L2(user_embs)\n",
    "    #faiss.normalize_L2(item_embs)\n",
    "    index.add(item_embs)  # Build an index from the item vectors\n",
    "    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)  # Query with user vectors to find the top-k most\n",
    "\n",
    "    user_recall_items_dict = defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_user_model_input['user_id'], sim, idx)):\n",
    "        target_raw_id = user_index_2_rawid[target_idx]\n",
    "        # Starting from 1 is to remove the item itself, so the final similar items obtained are topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_id = item_index_2_rawid[rele_idx]\n",
    "            user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, {})\\\n",
    "                                                                    .get(rele_raw_id, 0) + sim_value\n",
    "    user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in user_recall_items_dict.items()}\n",
    "    # Save the recall results\n",
    "    # Here, we get the recall results directly via the vector approach.\n",
    "    # This is different from the methods above, which only produced i2i and u2u similarity matrices\n",
    "    # and still required a collaborative filtering recall step to get the results.\n",
    "    # These recall results can be directly evaluated. For convenience, a single\n",
    "    # evaluation function can be written to evaluate all recall results.\n",
    "    pickle.dump(user_recall_items_dict, open(save_path + 'youtube_u2i_recall_dict.pkl', 'wb'))\n",
    "    return user_recall_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac31885-9f7e-4f52-9ed9-990f7a2664b2",
   "metadata": {},
   "source": [
    "#### YoutubeDNN recall and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cceed56-078f-40a3-b788-d048bca311b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [32:33<00:00, 102.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\danny\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:55: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 1016128 samples, validate on 254032 samples\n",
      "Epoch 1/30\n",
      "1016128/1016128 [==============================] - 169s 166us/sample - loss: 3.9269 - val_loss: 3.3179\n",
      "Epoch 2/30\n",
      "1016128/1016128 [==============================] - 183s 180us/sample - loss: 3.3996 - val_loss: 3.5891\n",
      "Epoch 3/30\n",
      "1016128/1016128 [==============================] - 184s 181us/sample - loss: 3.2787 - val_loss: 3.8667\n",
      "Epoch 4/30\n",
      "1016128/1016128 [==============================] - 179s 176us/sample - loss: 3.0007 - val_loss: 3.8141\n",
      "Epoch 5/30\n",
      "1016128/1016128 [==============================] - 173s 171us/sample - loss: 2.7218 - val_loss: 3.9326\n",
      "Epoch 6/30\n",
      "1016128/1016128 [==============================] - 161s 158us/sample - loss: 2.5224 - val_loss: 3.8504\n",
      "Epoch 7/30\n",
      "1016128/1016128 [==============================] - 166s 163us/sample - loss: 2.2885 - val_loss: 3.6770\n",
      "Epoch 8/30\n",
      "1016128/1016128 [==============================] - 164s 161us/sample - loss: 2.1395 - val_loss: 3.6714\n",
      "Epoch 9/30\n",
      "1016128/1016128 [==============================] - 135s 133us/sample - loss: 1.9818 - val_loss: 3.7156\n",
      "Epoch 10/30\n",
      "1016128/1016128 [==============================] - 136s 133us/sample - loss: 1.8533 - val_loss: 3.5782\n",
      "Epoch 11/30\n",
      "1016128/1016128 [==============================] - 136s 134us/sample - loss: 1.7821 - val_loss: 3.5052\n",
      "Epoch 12/30\n",
      "1016128/1016128 [==============================] - 137s 134us/sample - loss: 1.6626 - val_loss: 3.4641\n",
      "Epoch 13/30\n",
      "1016128/1016128 [==============================] - 137s 135us/sample - loss: 1.6044 - val_loss: 3.4009\n",
      "Epoch 14/30\n",
      "1016128/1016128 [==============================] - 135s 133us/sample - loss: 1.5692 - val_loss: 3.3169\n",
      "Epoch 15/30\n",
      "1016128/1016128 [==============================] - 138s 135us/sample - loss: 1.5110 - val_loss: 3.2584\n",
      "Epoch 16/30\n",
      "1016128/1016128 [==============================] - 161s 159us/sample - loss: 1.4691 - val_loss: 3.2851\n",
      "Epoch 17/30\n",
      "1016128/1016128 [==============================] - 132s 130us/sample - loss: 1.4435 - val_loss: 3.2210\n",
      "Epoch 18/30\n",
      "1016128/1016128 [==============================] - 140s 137us/sample - loss: 1.4022 - val_loss: 3.2241\n",
      "Epoch 19/30\n",
      "1016128/1016128 [==============================] - 141s 139us/sample - loss: 1.3561 - val_loss: 3.1743\n",
      "Epoch 20/30\n",
      "1016128/1016128 [==============================] - 130s 128us/sample - loss: 1.3382 - val_loss: 3.1785\n",
      "Epoch 21/30\n",
      "1016128/1016128 [==============================] - 133s 130us/sample - loss: 1.3080 - val_loss: 3.1253\n",
      "Epoch 22/30\n",
      "1016128/1016128 [==============================] - 138s 136us/sample - loss: 1.2752 - val_loss: 3.1429\n",
      "Epoch 23/30\n",
      "1016128/1016128 [==============================] - 143s 141us/sample - loss: 1.2926 - val_loss: 3.1130\n",
      "Epoch 24/30\n",
      "1016128/1016128 [==============================] - 139s 137us/sample - loss: 1.2516 - val_loss: 3.1060\n",
      "Epoch 25/30\n",
      "1016128/1016128 [==============================] - 131s 129us/sample - loss: 1.2137 - val_loss: 3.1219\n",
      "Epoch 26/30\n",
      "1016128/1016128 [==============================] - 137s 135us/sample - loss: 1.2038 - val_loss: 3.1044\n",
      "Epoch 27/30\n",
      "1016128/1016128 [==============================] - 143s 141us/sample - loss: 1.2136 - val_loss: 3.1132\n",
      "Epoch 28/30\n",
      "1016128/1016128 [==============================] - 145s 143us/sample - loss: 1.1883 - val_loss: 3.1432\n",
      "Epoch 29/30\n",
      "1016128/1016128 [==============================] - 137s 135us/sample - loss: 1.1840 - val_loss: 3.1393\n",
      "Epoch 30/30\n",
      "1016128/1016128 [==============================] - 137s 135us/sample - loss: 1.1601 - val_loss: 3.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "675899it [01:14, 9098.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 10  | hit_num: 75  | hit_rate: 0.00041  | user_num: 181592\n",
      "topk: 20  | hit_num: 142  | hit_rate: 0.00078  | user_num: 181592\n",
      "topk: 30  | hit_num: 199  | hit_rate: 0.0011  | user_num: 181592\n",
      "topk: 40  | hit_num: 257  | hit_rate: 0.00142  | user_num: 181592\n",
      "topk: 50  | hit_num: 308  | hit_rate: 0.0017  | user_num: 181592\n"
     ]
    }
   ],
   "source": [
    "# Since we need to evaluate recall here, we have extracted the last click from the training set.\n",
    "metric_recall = True\n",
    "\n",
    "if not metric_recall:\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(all_click_df, topk=50)\n",
    "else:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(trn_hist_click_df, topk=50)\n",
    "    # Evaluate recall performance\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_recall'], trn_last_click_df, topk=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3400a-26f7-466a-af5d-7eb70f0a6482",
   "metadata": {},
   "source": [
    "### 7.2 Item-based Collaborative Filtering Recall\n",
    "Above, we used collaborative filtering and embedding retrieval to obtain similarity matrices for articles. Below, we'll apply the concept of collaborative filtering to recall articles that are similar to a user's historical clicks.\n",
    "\n",
    "In this recall process, we also use an association rule-based approach:\n",
    "\n",
    "1. Weight based on the order of similar and previously clicked articles: The order in which a user clicks on articles is considered for weighting.\n",
    "\n",
    "2. Weight based on article creation time: We assign a weight based on the time difference between the creation dates of the similar article and the historically clicked article.\n",
    "\n",
    "3. Weight based on article content similarity: Embeddings are used to calculate the similarity between articles. However, it is important to note that embeddings are not calculated for every possible pair of items. Therefore, special handling is required if a similar article and a historically clicked article do not have a pre-computed similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf56f16d-bb58-42ab-b38e-63ddad22a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based Collaborative Filtering Recall (i2i)\n",
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "    Performs item-based collaborative filtering for recall.\n",
    "    :param user_id: The user's ID.\n",
    "    :param user_item_time_dict: A dictionary of user's clicked article sequences, keyed by user ID.\n",
    "                                 Example: {user1: [(item1, time1), (item2, time2)..]...}\n",
    "    :param i2i_sim: A dictionary representing the article similarity matrix.\n",
    "    :param sim_item_topk: An integer, selecting the top-k most similar articles to the current article.\n",
    "    :param recall_item_num: An integer, the final number of articles to recall.\n",
    "    :param item_topk_click: A list of the most-clicked articles, used to fill in if not enough candidates are found.\n",
    "    :param emb_i2i_sim: A dictionary for the article similarity matrix based on content embeddings.\n",
    "\n",
    "    :return: A list of recalled articles and their scores: [(item1, score1), (item2, score2)...]\n",
    "    \"\"\"\n",
    "    # Get the user's historical interaction articles\n",
    "    user_hist_items = user_item_time_dict[user_id]\n",
    "    user_hist_items_ = {user_id for user_id, _ in user_hist_items}\n",
    "\n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "            if j in user_hist_items_:\n",
    "                continue\n",
    "            # Weight based on the difference in article creation times\n",
    "            created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "            # Weight based on the position of the historical article in the user's sequence\n",
    "            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n",
    "\n",
    "            content_weight = 1.0\n",
    "            if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                content_weight += emb_i2i_sim[i][j]\n",
    "            if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                content_weight += emb_i2i_sim[j][i]\n",
    "\n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += created_time_weight * loc_weight * content_weight * wij\n",
    "\n",
    "    # If there are fewer than the required number of items, fill with popular items\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items():  # The filled item should not be in the original list\n",
    "                continue\n",
    "            item_rank[item] = - i - 100   # Assign a low negative score\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "\n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "\n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffb20b-f829-4467-9fbc-75f037faf433",
   "metadata": {},
   "source": [
    "#### itemcf sim recall and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "132a87d2-90f4-421a-b30c-0dbb44746e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [30:42<00:00, 108.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 10  | hit_num: 97853  | hit_rate: 0.48927  | user_num: 200000\n",
      "topk: 20  | hit_num: 123349  | hit_rate: 0.61674  | user_num: 200000\n",
      "topk: 30  | hit_num: 136999  | hit_rate: 0.685  | user_num: 200000\n",
      "topk: 40  | hit_num: 146051  | hit_rate: 0.73025  | user_num: 200000\n",
      "topk: 50  | hit_num: 152664  | hit_rate: 0.76332  | user_num: 200000\n"
     ]
    }
   ],
   "source": [
    "# First, perform Item-based Collaborative Filtering (itemcf) recall. \n",
    "# The last click is extracted for the purpose of recall evaluation.\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "emb_i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "sim_item_topk = 50\n",
    "recall_item_num = 50\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \\\n",
    "                                                        i2i_sim, sim_item_topk, recall_item_num, \\\n",
    "                                                        item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['itemcf_sim_itemcf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['itemcf_sim_itemcf_recall'], open(save_path + 'itemcf_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # Evaluate recall performance\n",
    "    metrics_recall(user_multi_recall_dict['itemcf_sim_itemcf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab084d2-d26b-488d-91a5-6af03f01145c",
   "metadata": {},
   "source": [
    "### 7.3 Item Embedding Similarity Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb3912d-3d33-4ecb-afb5-1c440ce71002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [00:59<00:00, 3347.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 10  | hit_num: 3986  | hit_rate: 0.01993  | user_num: 200000\n",
      "topk: 20  | hit_num: 12376  | hit_rate: 0.06188  | user_num: 200000\n",
      "topk: 30  | hit_num: 19134  | hit_rate: 0.09567  | user_num: 200000\n",
      "topk: 40  | hit_num: 26550  | hit_rate: 0.13275  | user_num: 200000\n",
      "topk: 50  | hit_num: 33422  | hit_rate: 0.16711  | user_num: 200000\n"
     ]
    }
   ],
   "source": [
    "# The last click is extracted for the purpose of recall evaluation.\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 50\n",
    "recall_item_num = 50\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['embedding_sim_item_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['embedding_sim_item_recall'], open(save_path + 'embedding_sim_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # Evaluate recall performance\n",
    "    metrics_recall(user_multi_recall_dict['embedding_sim_item_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53191c6-1e09-4d86-900b-e1c23942ea68",
   "metadata": {},
   "source": [
    "### 7.4 User-based Collaborative Filtering Recall\n",
    "The core idea behind user-based collaborative filtering is to recommend articles to a user that were clicked by other similar users. Since this involves the historical articles of similar users, we can still add some association rules to weight the articles that are likely to be clicked. The association rules used here primarily consider the relationship between the historical articles of similar users and the articles of the user being recommended to. The relationship here can directly borrow from the approach used in item-based collaborative filtering. This process accumulates weights for the items being recommended.\n",
    "\n",
    "Here are the relationships used for weighting, along with the corresponding code:\n",
    "\n",
    "- Calculate the similarity, creation time difference, and relative position sum between the articles clicked by the user being recommended to and the articles clicked by similar users. This sum serves as their respective weights.\n",
    "\n",
    "Note: Calculating user-to-user similarity in UserCF is too memory-intensive. Therefore, we did not run it on the full dataset and instead used a sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "642d6f14-6887-4f79-a132-21fb3340af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-based Collaborative Filtering Recall (u2u)\n",
    "def user_based_recommend(user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num,\n",
    "                         item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "    Performs user-based collaborative filtering for recall.\n",
    "    :param user_id: The user's ID.\n",
    "    :param user_item_time_dict: A dictionary, keyed by user ID, containing the user's clicked article sequence based on click time.\n",
    "                                 Example: {user1: [(item1, time1), (item2, time2)..]...}\n",
    "    :param u2u_sim: A dictionary representing the user similarity matrix.\n",
    "    :param sim_user_topk: An integer, selecting the top-k most similar users to the current user.\n",
    "    :param recall_item_num: An integer, the final number of articles to recall.\n",
    "    :param item_topk_click: A list of the most-clicked articles, used to fill in if not enough candidates are found.\n",
    "    :param item_created_time_dict: A dictionary of article creation times.\n",
    "    :param emb_i2i_sim: A dictionary for the article similarity matrix based on content embeddings.\n",
    "\n",
    "    :return: A list of recalled articles and their scores: [(item1, score1), (item2, score2)...]\n",
    "    \"\"\"\n",
    "    # Get user's historical interactions\n",
    "    user_item_time_list = user_item_time_dict[user_id]    #  [(item1, time1), (item2, time2)..]\n",
    "    user_hist_items = set([i for i, t in user_item_time_list])  # A user may interact with an article multiple times; this removes duplicates.\n",
    "\n",
    "    items_rank = {}\n",
    "    for sim_u, wuv in sorted(u2u_sim[user_id].items(), key=lambda x: x[1], reverse=True)[:sim_user_topk]:\n",
    "        for i, click_time in user_item_time_dict[sim_u]:\n",
    "            if i in user_hist_items:\n",
    "                continue\n",
    "            items_rank.setdefault(i, 0)\n",
    "\n",
    "            loc_weight = 1.0\n",
    "            content_weight = 1.0\n",
    "            created_time_weight = 1.0\n",
    "\n",
    "            # Apply association rules based on the user's historical articles\n",
    "            for loc, (j, click_time) in enumerate(user_item_time_list):\n",
    "                # Weight based on the relative position of the historical article\n",
    "                loc_weight += 0.9 ** (len(user_item_time_list) - loc)\n",
    "                # Weight based on content similarity\n",
    "                if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[i][j]\n",
    "                if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[j][i]\n",
    "\n",
    "                # Weight based on creation time difference\n",
    "                created_time_weight += np.exp(0.8 * np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "\n",
    "            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv\n",
    "\n",
    "    # If there are not enough recalled items, fill with popular items\n",
    "    if len(items_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in items_rank.items():  # The filled item should not be in the original list\n",
    "                continue\n",
    "            items_rank[item] = - i - 100  # Assign a low negative score\n",
    "            if len(items_rank) == recall_item_num:\n",
    "                break\n",
    "\n",
    "    items_rank = sorted(items_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "\n",
    "    return items_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba55233-749c-415f-af0c-7ddbca7d1d72",
   "metadata": {},
   "source": [
    "#### usercf sim recall and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98d666-e53f-42f0-883b-5e16a74abdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last click is extracted for the purpose of recall evaluation.\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "u2u_sim = pickle.load(open(save_path + 'usercf_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "sim_user_topk = 20\n",
    "recall_item_num = 10\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "pickle.dump(user_recall_items_dict, open(save_path + 'usercf_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # Evaluate recall performance\n",
    "    metrics_recall(user_recall_items_dict, trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ddc799-82cc-4346-9e1e-738706d39554",
   "metadata": {},
   "source": [
    "### 7.5 User Embedding Similarity Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569e0f0-195d-4c82-96f7-c945ba81d5bb",
   "metadata": {},
   "source": [
    "Although we didn't directly run the UserCF similarity calculation for the full dataset due to its high memory cost, we can still validate the user-based collaborative filtering approach. Below, we'll use the user embeddings generated from the YouTube DNN process to perform a vector search and find the top-k most similar users for each user. Then, we will use the resulting u2u (user-to-user) similarity matrix to perform UserCF recall. The code for this is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d850f5a7-5d44-49ae-a47d-a65fd4f28b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the embedding method to get the user-to-user (u2u) similarity matrix\n",
    "# topk refers to the number of most similar users returned by Faiss for each user\n",
    "def u2u_embdding_sim(user_emb_dict, save_path, topk):\n",
    "    user_list = []\n",
    "    user_emb_list = []\n",
    "    \n",
    "    for user_id, user_emb in user_emb_dict.items():\n",
    "        user_list.append(user_id)\n",
    "        user_emb_list.append(user_emb)\n",
    "\n",
    "    user_index_2_rawid_dict = {k: v for k, v in zip(range(len(user_list)), user_list)}\n",
    "\n",
    "    user_emb_np = np.array(user_emb_list, dtype=np.float32)\n",
    "    # Build the Faiss index\n",
    "    user_index = faiss.IndexFlatIP(user_emb_np.shape[1])\n",
    "    user_index.add(user_emb_np)\n",
    "    # Perform a similarity search, returning the top-k items and their similarities for each vector in the index\n",
    "    sim, idx = user_index.search(user_emb_np, topk)  # Returns a list\n",
    "\n",
    "    # Save the vector retrieval results with a correspondence to the original IDs\n",
    "    user_sim_dict = defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(user_emb_np)), sim, idx)):\n",
    "        target_raw_id = user_index_2_rawid_dict[target_idx]\n",
    "        # Start from 1 to remove the user's own ID, so the final similar users obtained are topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_id = user_index_2_rawid_dict[rele_idx]\n",
    "            user_sim_dict[target_raw_id][rele_raw_id] = user_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "\n",
    "    # Save the u2u similarity matrix\n",
    "    pickle.dump(user_sim_dict, open(save_path + 'youtube_u2u_sim.pkl', 'wb'))\n",
    "    return user_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "037d4de9-4bc5-4079-b46e-d1162c7b3904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200000it [00:08, 23078.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the user embeddings generated from the YouTubeDNN process, then use Faiss to calculate\n",
    "# the similarity between users.\n",
    "# Note: The user embeddings obtained here may not be optimal, as YouTubeDNN trains user embeddings\n",
    "# using user click sequences. If the sequences are generally short, the results may not be very good.\n",
    "user_emb_dict = pickle.load(open(save_path + 'user_youtube_emb.pkl', 'rb'))\n",
    "u2u_sim = u2u_embdding_sim(user_emb_dict, save_path, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f275f6d1-4f75-42e1-bb32-bcd8dc4f8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [02:39<00:00, 1250.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 10  | hit_num: 2999  | hit_rate: 0.01499  | user_num: 200000\n",
      "topk: 20  | hit_num: 7125  | hit_rate: 0.03562  | user_num: 200000\n",
      "topk: 30  | hit_num: 15088  | hit_rate: 0.07544  | user_num: 200000\n",
      "topk: 40  | hit_num: 24493  | hit_rate: 0.12247  | user_num: 200000\n",
      "topk: 50  | hit_num: 33096  | hit_rate: 0.16548  | user_num: 200000\n"
     ]
    }
   ],
   "source": [
    "# Use the recall evaluation function to verify the performance of the current recall method.\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "u2u_sim = pickle.load(open(save_path + 'youtube_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "sim_user_topk = 50\n",
    "recall_item_num = 50\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['youtubednn_usercf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['youtubednn_usercf_recall'], open(save_path + 'youtubednn_usercf_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # Evaluate recall performance\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_usercf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf5ddc-99d1-47a2-bd3c-00a7a0f090eb",
   "metadata": {},
   "source": [
    "## 8. Cold-start Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110d957-fe7a-40f2-806f-c66905036776",
   "metadata": {},
   "source": [
    "Cold-start is a classic problem in recommendation systems. It occurs when the system lacks sufficient data to make accurate recommendations. The problem can be broken down into three main categories:\n",
    "\n",
    "- **Item Cold-Start**: This happens with new articles that have no click history. The challenge is to figure out how to recommend this new content to users. In our scenario, any article not present in the log data can be considered a cold-start article.\n",
    "\n",
    "- **User Cold-Start**: This affects new users who have just joined the platform and have no interaction history. The system doesn't know what to recommend. In our case, a user in the test set who has only clicked once could be considered a cold-start user.\n",
    "\n",
    "- **System Cold-Start**: This is a combination of the above two problems. It happens when a brand new platform has no historical data for either users or articles.\n",
    "\n",
    "#### Our Current Cold-Start Challenge\n",
    "In our current dataset, we're facing significant cold-start issues: \n",
    "1. There are over 300,000 articles in the total library, but only around 30,000 of them have any click data. This means a huge number of articles have no interaction history, which is the definition of article cold-start.\n",
    "2. We also see signs of user cold-start: nearly 20% of the users in our test set have only one click. Recommending content to these users is difficult because we have very little to go on.\n",
    "\n",
    "While both of these problems are important, we'll focus primarily on article cold-start and its solutions. The user cold-start problem can be solved by recommending top articles, or passing the user embedding vector to the user tower of a two-tower model (e.g. YoutubeDNN) and retrieve most similar item embedding vectors via ANN (faiss).\n",
    "\n",
    "#### A Proposed Solution for Article Cold-Start\n",
    "Our goal isn't just to find articles that don't have click data, but to find articles that a user might actually click. We can treat this as a specialized recall strategy. Here's a possible approach:\n",
    "\n",
    "1. Embedding-based Recall: First, we can use item embeddings to find a set of articles that are similar to the ones a user has historically clicked on.\n",
    "\n",
    "2. Rule-based Filtering: From this recalled set, we can apply some simple rules to filter the articles. For example, we could keep only articles that match the user's preferred topics or reading length. We should also prioritize articles that were created around the same time as the user's last click, as this can capture a user's interest in a timely event.\n",
    "\n",
    "This approach is different from a standard embedding-based recall. Our goal is to find similar articles that have not appeared in the log data. This means we need to recall a larger number of candidates initially, as many will be filtered out by our cold-start rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcda73bd-bd02-4b6d-a438-a251135c80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [01:18<00:00, 2549.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# First, perform ItemCF recall.\n",
    "trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 150\n",
    "recall_item_num = 100  # Recall a slightly larger number of articles to make subsequent filtering easier.\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n",
    "                                                        recall_item_num, item_topk_click,item_created_time_dict, emb_i2i_sim)\n",
    "pickle.dump(user_recall_items_dict, open(save_path + 'cold_start_items_raw_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ead7a832-ee9d-4e2f-90e9-231136c8fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter articles based on rules\n",
    "# Retain articles with topics similar to the user's historical browsing topics\n",
    "# Retain articles with a word count that is not significantly different from the user's historical articles\n",
    "# Retain articles created on the same day as the user's last click\n",
    "# Return the final results sorted by similarity score\n",
    "\n",
    "def get_click_article_ids_set(click_df):\n",
    "    return set(click_df.click_article_id.values)\n",
    "\n",
    "def cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                     user_last_item_created_time_dict, item_type_dict, item_words_dict,\n",
    "                     item_created_time_dict, click_article_ids_set, recall_item_num):\n",
    "    \"\"\"\n",
    "    Recalls articles for a cold-start scenario.\n",
    "    :param user_recall_items_dict: A dictionary of many articles recalled based on embedding similarity.\n",
    "                                    Example: {user1: [(item1, item2), ..], }\n",
    "    :param user_hist_item_typs_dict: A dictionary mapping user IDs to the topics of their clicked articles.\n",
    "    :param user_hist_item_words_dict: A dictionary mapping user IDs to the word count of their historical articles.\n",
    "    :param user_last_item_created_time_dict: A dictionary mapping user IDs to the creation time of their last clicked article.\n",
    "    :param item_type_dict: A dictionary mapping article IDs to their topics.\n",
    "    :param item_words_dict: A dictionary mapping article IDs to their word counts.\n",
    "    :param item_created_time_dict: A dictionary mapping article IDs to their creation times.\n",
    "    :param click_article_ids_set: A set of articles clicked by users (i.e., articles that have appeared in the log).\n",
    "    :param recall_item_num: The number of articles to recall. This refers to articles that have not appeared in the log data.\n",
    "    \"\"\"\n",
    "    cold_start_user_items_dict = {}\n",
    "    for user, item_list in tqdm(user_recall_items_dict.items()):\n",
    "        cold_start_user_items_dict.setdefault(user, [])\n",
    "\n",
    "        # Get historical article information\n",
    "        hist_item_type_set = user_hist_item_typs_dict[user]\n",
    "        hist_mean_words = user_hist_item_words_dict[user]\n",
    "        hist_last_item_created_time = user_last_item_created_time_dict[user]\n",
    "        hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)\n",
    "        \n",
    "        for item, score in item_list:\n",
    "            # Get information for the current recalled article\n",
    "            curr_item_type = item_type_dict[item]\n",
    "            curr_item_words = item_words_dict[item]\n",
    "            curr_item_created_time = item_created_time_dict[item]\n",
    "            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)\n",
    "\n",
    "            # First, the article must not have been clicked by the user. Then, filter based on article topic,\n",
    "            # word count, and creation time.\n",
    "            if curr_item_type not in hist_item_type_set or \\\n",
    "                item in click_article_ids_set or \\\n",
    "                abs(curr_item_words - hist_mean_words) > 200 or \\\n",
    "                abs((curr_item_created_time - hist_last_item_created_time).days) > 90:\n",
    "                continue\n",
    "\n",
    "            cold_start_user_items_dict[user].append((item, score))      # {user1: [(item1, score1), (item2, score2)..]...}\n",
    "\n",
    "    # Control the number of cold-start recalled articles\n",
    "    cold_start_user_items_dict = {k: sorted(v, key=lambda x:x[1], reverse=True)[:recall_item_num] \\\n",
    "                                  for k, v in cold_start_user_items_dict.items()}\n",
    "\n",
    "    pickle.dump(cold_start_user_items_dict, open(save_path + 'cold_start_user_items_dict.pkl', 'wb'))\n",
    "\n",
    "    return cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8ff7046-2a9c-41ec-a486-ce8f9ca1a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [00:26<00:00, 7615.05it/s]\n"
     ]
    }
   ],
   "source": [
    "all_click_df_ = all_click_df.copy()\n",
    "all_click_df_ = all_click_df_.merge(item_info_df, how='left', on='click_article_id')\n",
    "user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)\n",
    "click_article_ids_set = get_click_article_ids_set(all_click_df)\n",
    "\n",
    "# Note: We are using many rules to filter cold-start articles here,\n",
    "# so the previous recall stage should have recalled a much larger number of articles.\n",
    "# Otherwise, it's easy to have no articles left after filtering.\n",
    "cold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \\\n",
    "                                              item_created_time_dict, click_article_ids_set, recall_item_num)\n",
    "\n",
    "user_multi_recall_dict['cold_start_recall'] = cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb858d4-85c0-44ee-9335-03966c0ae6af",
   "metadata": {},
   "source": [
    "## 9. Multi-Channel Recall Merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a3f8d-810c-40f7-a224-7bf638bd0573",
   "metadata": {},
   "source": [
    "Multi-channel recall merging combines the lists of articles retrieved by all of our recall strategies. Below is a summary of all the recall results:\n",
    "\n",
    "1. Recall based on the item-to-item similarity calculated by Item-based Collaborative Filtering (itemcf).\n",
    "\n",
    "2. Recall based on the item-to-item similarity calculated using embeddings.\n",
    "\n",
    "3. YouTube DNN recall.\n",
    "\n",
    "4. Recall based on the user-to-user similarity calculated from YouTube DNN embeddings.\n",
    "\n",
    "5. Recall from the cold-start strategy.\n",
    "\n",
    "Note:\n",
    "Some strategies may perform better than others. Therefore, we can manually define different weights for the results from each recall channel to perform a final similarity-based fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f354c7f-d960-4a4e-bfb2-1ef499341a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25):\n",
    "    final_recall_items_dict = {}\n",
    "\n",
    "    # Normalize the scores for each user for each recall method,\n",
    "    # to allow for score summation across different recall channels.\n",
    "    def norm_user_recall_items_sim(sorted_item_list):\n",
    "        # If there are no articles or only one, return the list directly.\n",
    "        # This can happen when the number of cold-start recalled articles is too small,\n",
    "        # and all of them are filtered out. We could implement other strategies here.\n",
    "        if len(sorted_item_list) < 2:\n",
    "            return sorted_item_list\n",
    "\n",
    "        min_sim = sorted_item_list[-1][1]\n",
    "        max_sim = sorted_item_list[0][1]\n",
    "\n",
    "        norm_sorted_item_list = []\n",
    "        for item, score in sorted_item_list:\n",
    "            if max_sim > 0:\n",
    "                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0\n",
    "            else:\n",
    "                norm_score = 0.0\n",
    "            norm_sorted_item_list.append((item, norm_score))\n",
    "\n",
    "        return norm_sorted_item_list\n",
    "\n",
    "    print('Merging multi-channel recall...')\n",
    "    for method, user_recall_items in tqdm(user_multi_recall_dict.items()):\n",
    "        print(method + '...')\n",
    "        # When calculating the final recall results, a weight can be set for each recall method.\n",
    "        if weight_dict == None:\n",
    "            recall_method_weight = 1\n",
    "        else:\n",
    "            recall_method_weight = weight_dict[method]\n",
    "\n",
    "        for user_id, sorted_item_list in user_recall_items.items(): # è¿›è¡Œå½’ä¸€åŒ–\n",
    "            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)\n",
    "\n",
    "        for user_id, sorted_item_list in user_recall_items.items():\n",
    "            # print('user_id')\n",
    "            final_recall_items_dict.setdefault(user_id, {})\n",
    "            for item, score in sorted_item_list:\n",
    "                final_recall_items_dict[user_id].setdefault(item, 0)\n",
    "                final_recall_items_dict[user_id][item] += recall_method_weight * score\n",
    "\n",
    "    final_recall_items_dict_rank = {}\n",
    "    # When combining multiple recall channels, we can also control the final number of recalled items.\n",
    "    for user, recall_item_dict in final_recall_items_dict.items():\n",
    "        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # Save the final dictionary of merged recall results locally.\n",
    "    pickle.dump(final_recall_items_dict_rank, open(os.path.join(save_path, 'final_recall_items_dict.pkl'),'wb'))\n",
    "\n",
    "    return final_recall_items_dict_rank  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46d5faa9-d01a-4824-a900-2584195a5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we directly assigned the same weight to all multi-channel recall methods.\n",
    "# These parameter values can be adjusted based on previous recall evaluations.\n",
    "weight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n",
    "               'embedding_sim_item_recall': 1.0,\n",
    "               'youtubednn_recall': 1.0,\n",
    "               'youtubednn_usercf_recall': 1.0,\n",
    "               'cold_start_recall': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4959ff45-6cb1-4e99-a49a-b33cce010f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging multi-channel recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemcf_sim_itemcf_recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                   | 1/5 [00:20<01:20, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_sim_item_recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                  | 2/5 [00:36<00:53, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtubednn_recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 3/5 [01:14<00:54, 27.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtubednn_usercf_recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 4/5 [01:41<00:27, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold_start_recall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:07<00:00, 25.49s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# After the final merge, N items are recalled for each user for ranking\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_recall_items_dict_rank \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_recall_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_multi_recall_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 51\u001b[0m, in \u001b[0;36mcombine_recall_results\u001b[1;34m(user_multi_recall_dict, weight_dict, topk)\u001b[0m\n\u001b[0;32m     48\u001b[0m     final_recall_items_dict_rank[user] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(recall_item_dict\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:topk]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the final dictionary of merged recall results locally.\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_recall_items_dict_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_recall_items_dict.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_recall_items_dict_rank\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# After the final merge, N items are recalled for each user for ranking\n",
    "final_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, weight_dict, topk=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ce960-8d58-4b43-9694-e4c36536d570",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "We implemented and evaluated several recall strategies:\n",
    "\n",
    "1. Item-based Collaborative Filtering with association rules.\n",
    "2. User-based Collaborative Filtering with association rules.\n",
    "3. YouTube DNN Recall.\n",
    "4. Cold-Start Recall with content-based rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74566117-9952-4a4c-891f-6d7580a6910c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
