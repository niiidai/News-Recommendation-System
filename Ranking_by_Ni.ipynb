{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e548f4dc-ddc6-43bb-8ed0-d2bc769461ed",
   "metadata": {},
   "source": [
    "# 📰 News Recommendation System Part 4 - Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e43f09-32f3-44b2-9ad9-a4bd1e9d7815",
   "metadata": {},
   "source": [
    "## 📘 Project Introduction\n",
    "This project explores user behavior prediction in a news recommendation scenario. The goal is to build a model that can predict a user's future click behavior based on their historical browsing and clicking behavior data, specifically the last news article they clicked on.\n",
    "\n",
    "The setting is inspired by a real-world news app, where delivering timely, relevant content is essential for user engagement. This project aims to simulate a practical recommender system, combining business intuition with machine learning techniques to address a realistic problem in the content recommendation space.\n",
    "\n",
    "## 📊 Data Overview\n",
    "The dataset contains user interaction data from a large-scale news platform, including:\n",
    "- 300,000 users\n",
    "- ~3 million clicks\n",
    "- 360,000+ unique news articles; each news article is represented by a pre-trained embedding vector, capturing semantic relationships between articles.\n",
    "\n",
    "We extracted click log data from 200,000 users as the training set, 50,000 users as test set A, and 50,000 users as test set B.\n",
    "\n",
    "## 📄 Data Tables\n",
    "\n",
    "- train_click_log.csv: Training set user click logs\n",
    "- testA_click_log.csv: Test set user click logs\n",
    "- articles.csv: News article information data table\n",
    "- articles_emb.csv: Embedding vector representation of news articles\n",
    "\n",
    "|        **Field**        |         **Description**          |\n",
    "| :---------------------: | :------------------------------: |\n",
    "|         user_id         |              User ID             |\n",
    "|    click_article_id     |            Clicked article ID    |\n",
    "|     click_timestamp     |            Click timestamp        |\n",
    "|    click_environment    |             Click environment     |\n",
    "|    click_deviceGroup    |            Click device group     |\n",
    "|        click_os         |           Click operating system  |\n",
    "|      click_country      |             Click city            |\n",
    "|      click_region       |             Click region          |\n",
    "|   click_referrer_type   |           Click source type       |\n",
    "|       article_id        | Article ID, corresponding to click_article_id |\n",
    "|       category_id       |            Article type ID        |\n",
    "|      created_at_ts      |          Article creation timestamp |\n",
    "|       words_count       |             Article word count     |\n",
    "| emb_1,emb_2,...,emb_249 |      Article embedding vector representation |\n",
    "\n",
    "## 📏 Evaluation Metrics\n",
    "The final recommendation for each user will include five recommended articles, sorted by click probability.\n",
    "\n",
    "For example, for user1, our recommendation would be:\n",
    "> user1, article1, article2, article3, article4, article5.\n",
    "\n",
    "There is only one correct answer for each user's last clicked article, so we check if any of the recommended five articles match the actual answer. We will use **mean reciprocal rank** as the evaluation metric. The formula is as follows:\n",
    "$$\n",
    "score(user) = \\sum_{k=1}^5 \\frac{s(user, k)}{k}\n",
    "$$\n",
    "\n",
    "If article1 is the actual article clicked by the user, then s(user1, 1) = 1, and s(user1, 2-4) are all 0. If article2 is the article clicked by the user, then s(user, 2) = 1/2, and s(user, 1, 3, 4, 5) are all 0. Thus, score(user) = the reciprocal of the rank at which the match occurs. If there are no matches, score(user1) = 0. This is reasonable because we want hits to be as high-ranking as possible, which yields a higher score.\n",
    "\n",
    "## 💡 Project Understanding\n",
    "The goal of this project is to **predict the last news article a user clicked, based on their historical browsing data**. Unlike traditional structured prediction problems, this is more aligned with real-world recommendation systems, using raw user click logs rather than neatly labeled data.\n",
    "\n",
    "To approach this, I framed the task as a **supervised learning** problem by transforming user-article interactions into \"features + labels\" training data. The core idea is to predict the likelihood of a user clicking a given article, turning this into a click-through rate (CTR) prediction task. This reframing allows for the use of **classification models**—starting with simple baselines like logistic regression and moving toward deep learning approaches.\n",
    "\n",
    "Now, we have converted this problem into a classification problem, where the classification label is whether the user will click on a particular article. The features of the classification problem will include the user and the article. We need to train a classification model to predict the probability of a particular user clicking on a specific article. This raises several additional questions:\n",
    "- How to create training and testing datasets?\n",
    "- What specific features can we leverage?\n",
    "- What models can we attempt?\n",
    "- With 360,000 articles and over 200,000 users, what strategies do we have to reduce the problem's scale? How do we make the final predictions?\n",
    "\n",
    "**For the fourth part, we will use the features from Part 3 to train machine learning models to rank the recalled candidates and predict users' click through rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab32b1-1521-429a-b16f-224f987fa72f",
   "metadata": {},
   "source": [
    "## The Ranking Phase\n",
    "After the recall phase, we have successfully narrowed down a huge list of articles to a smaller, more manageable set of candidates for each user. Now, the goal is to use a machine learning model to rank these candidates and find the ones a user is most likely to click on.\n",
    "\n",
    "We'll use the rich features we've engineered—including user attributes, article attributes, and features derived from the relationship between the user and the articles—to predict a click probability for each candidate. The top-k articles with the highest predicted probabilities will be our final recommendation.\n",
    "\n",
    "### The Models\n",
    "We'll use three popular and powerful models for this ranking task:\n",
    "\n",
    "1. LGBM Ranking Model: A LightGBM model configured specifically for ranking.\n",
    "\n",
    "2. LGBM Classification Model: A LightGBM model configured for binary classification (to predict a click or no-click).\n",
    "\n",
    "3. DIN (Deep Interest Network): A deep learning classification model that is highly effective for e-commerce and recommendation tasks.\n",
    "\n",
    "### Model Ensembling\n",
    "After we get the output from these models, we won't just pick one. We'll use two classic model ensembling techniques to combine their strengths for even better performance:\n",
    "\n",
    "1. Weighted Fusion: We'll combine the output scores from each model using a weighted average. This lets us give more importance to the models that perform better.\n",
    "\n",
    "2. Stacking: We'll use a second, simpler model to take the outputs of our three ranking models and use them as its input features. This allows a meta-model to learn how to best combine the predictions of the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21025c-fcc0-4810-97c1-32e9e2babe8d",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07531f12-8207-4144-a570-40e4dfc0fad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import deepctr\n",
    "from deepctr.models import DIN\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be3a26-912c-4dc2-97cb-22cc8e12555a",
   "metadata": {},
   "source": [
    "## 2. Read Features for the Ranking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af90270-77c0-41f1-bacb-471b8e08ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, use this cell to load data\n",
    "from google.colab import drive\n",
    "\n",
    "# Connect to Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define file paths\n",
    "data_path = '/content/drive/MyDrive/Datasets/news-rec-sys/'\n",
    "save_path = '/content/drive/MyDrive/Datasets/news-rec-sys/temp_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69696e6-6a7c-4d75-bdb5-f7f906a37803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a local machine\n",
    "data_path = './data/' \n",
    "save_path = './data/temp_results/' # save temperary result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fed07-1879-4549-856c-63baedcc9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = False\n",
    "\n",
    "# When rereading the data, we found that 'click_article_id' is a float, so we convert it to an integer type.\n",
    "trn_user_item_feats_df = pd.read_csv(save_path + 'trn_user_item_feats_df.csv')\n",
    "trn_user_item_feats_df['click_article_id'] = trn_user_item_feats_df['click_article_id'].astype(int)\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df = pd.read_csv(save_path + 'val_user_item_feats_df.csv')\n",
    "    val_user_item_feats_df['click_article_id'] = val_user_item_feats_df['click_article_id'].astype(int)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "\n",
    "tst_user_item_feats_df = pd.read_csv(save_path + 'tst_user_item_feats_df.csv')\n",
    "tst_user_item_feats_df['click_article_id'] = tst_user_item_feats_df['click_article_id'].astype(int)\n",
    "\n",
    "# For convenience during feature engineering, we also give the test set an invalid label. We can simply delete it now.\n",
    "del tst_user_item_feats_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82802e51-4fef-4723-8880-993b6d3461d0",
   "metadata": {},
   "source": [
    "## 3. Save Ranking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275bdce-d47d-441d-a332-ba33943af22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(recall_df, topk=5, model_name=None):\n",
    "    # Sort the DataFrame by 'user_id' and 'pred_score'.\n",
    "    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n",
    "\n",
    "    # Add a 'rank' column to show the predicted rank for each user.\n",
    "    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "    # Check that each user has at least 'topk' articles.\n",
    "    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n",
    "    assert tmp.min() >= topk\n",
    "\n",
    "    # Delete the 'pred_score' column.\n",
    "    del recall_df['pred_score']\n",
    "\n",
    "    # Select the top 'topk' predictions for each user and reshape the DataFrame.\n",
    "    results = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n",
    "\n",
    "    # Rename the column names (return 5 recommended articles).\n",
    "    results.columns = [int(col) if isinstance(col, int) else col for col in results.columns.droplevel(0)]\n",
    "    results = results.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2',\n",
    "                                    3: 'article_3', 4: 'article_4', 5: 'article_5'})\n",
    "\n",
    "    # Generate a filename for the CSV file.\n",
    "    save_name = save_path + model_name + '_' + datetime.today().strftime('%m-%d') + '.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file without the index and with a header.\n",
    "    results.to_csv(save_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef87e43-d9f1-4eda-be39-9e9cc6212040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sim(sim_df, weight=0.0):\n",
    "    min_sim = sim_df.min()  # Calculate the minimum similarity value.\n",
    "    max_sim = sim_df.max()  # Calculate the maximum similarity value.\n",
    "\n",
    "    if max_sim == min_sim:\n",
    "        sim_df = sim_df.apply(lambda sim: 1.0)  # If max and min are equal, normalize all similarities to 1.0.\n",
    "    else:\n",
    "        sim_df = sim_df.apply(lambda sim: 1.0 * (sim - min_sim) / (max_sim - min_sim))  # 对相似度进行归一化\n",
    "\n",
    "    sim_df = sim_df.apply(lambda sim: sim + weight)  # Add a weight to the normalized similarity.\n",
    "\n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2123c-5aa5-45c5-afbd-d6827130b6d2",
   "metadata": {},
   "source": [
    "## 4. LightGBM Ranking Model\n",
    "The code in this section is designed to prevent data loss if an error occurs. It ensures that the original DataFrames remain unchanged by creating a copy for the ranking model's use.\n",
    "\n",
    "The code performs the following actions:\n",
    "\n",
    "- It copies the training set feature data (trn_user_item_feats_df) to a new DataFrame called trn_user_item_feats_df_rank_model.\n",
    "\n",
    "- If the script is in offline validation mode, it also copies the validation set feature data (val_user_item_feats_df) to a new DataFrame called val_user_item_feats_df_rank_model.\n",
    "\n",
    "- It copies the test set feature data (tst_user_item_feats_df) to a new DataFrame called tst_user_item_feats_df_rank_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346ba04-755d-47e1-bbe3-f02992093a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent data reloading if an error occurs in the middle of the process.\n",
    "trn_user_item_feats_df_rank_model = trn_user_item_feats_df.copy()\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_rank_model = val_user_item_feats_df.copy()\n",
    "\n",
    "tst_user_item_feats_df_rank_model = tst_user_item_feats_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5f033-afe1-4659-bb80-3b5dd48c59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Columns\n",
    "lgb_cols = ['sim0', 'time_diff0', 'word_diff0','sim_max', 'sim_min', 'sim_sum',\n",
    "            'sim_mean', 'score','click_size', 'click_time_diff_mean', 'active_level',\n",
    "            'click_environment','click_deviceGroup', 'click_os', 'click_country',\n",
    "            'click_region','click_referrer_type', 'user_time_hob1', 'user_time_hob2',\n",
    "            'words_hbo', 'category_id', 'created_at_ts','words_count', 'user_num', \n",
    "            'article_time_diff_mean', 'hot_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff9c76-8c85-4cb6-b4e7-23448575a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping for the Ranking Model\n",
    "trn_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\n",
    "g_train = trn_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\n",
    "    g_val = val_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2bda3-e470-46a8-a04e-35673f12800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM \n",
    "lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b8699-1fcc-4862-bbf1-41912733ac94",
   "metadata": {},
   "source": [
    "### LightGBM Ranking Model Parameters\n",
    "* `boosting_type`: The type of boosting algorithm. We're using 'gbdt', which stands for Gradient Boosting Decision Tree.\n",
    "* `num_leaves`: The maximum number of leaves in one tree. Setting this to 31 is a common default.\n",
    "* `reg_alpha (L1 Regularization)`: A coefficient for L1 regularization. We're setting it to 0.0, which means L1 regularization is not being used to control model complexity.\n",
    "* `reg_lambda (L2 Regularization)`: A coefficient for L2 regularization. We're setting it to 1, which means we are using L2 regularization.\n",
    "* `max_depth`: The maximum depth of each tree. We've set it to -1, which means there is no limit on the depth.\n",
    "* `n_estimators`: The number of trees in the model. We're training a total of 100 trees.\n",
    "* `subsample`: The fraction of the training data to be randomly sampled for each tree. We're using 0.7, or 70%.\n",
    "* `colsample_bytree`: The fraction of features (columns) to be randomly sampled for each tree. We're also using 0.7.\n",
    "* `subsample_freq`: The frequency at which subsampling occurs. Setting it to 1 means we'll use a new random subsample for every tree.\n",
    "* `learning_rate`: Controls the step size of each iteration. A smaller value like 0.01 means the model takes more gradual steps, which can improve accuracy.\n",
    "* `min_child_weight`: This is a pruning parameter that controls the minimum sum of weights of all data points in a child leaf. We've set it to 50 to help prevent the model from overfitting.\n",
    "* `random_state`: The random seed. Setting this to 2018 ensures that our results are reproducible.\n",
    "* `n_jobs`: The number of threads to use for training. We're using 16 threads to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef918d4-6de8-454f-a6a2-9e7e9ac437a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lighBGM Ranking model\n",
    "if offline:\n",
    "    lgb_ranker.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'], group=g_train,\n",
    "                eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])],\n",
    "                eval_group= [g_val], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, )\n",
    "else:\n",
    "    lgb_ranker.fit(trn_user_item_feats_df[lgb_cols], trn_user_item_feats_df['label'], group=g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c086-25d6-4ebf-9b5b-1377d54de6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "tst_user_item_feats_df['pred_score'] = lgb_ranker.predict(tst_user_item_feats_df[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "\n",
    "# Save these ranking results for later model ensembling\n",
    "tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path + 'lgb_ranker_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd44023-b81d-4e2b-a0d7-ac0f3fa8d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "save_results(rank_results, topk=5, model_name='lgb_ranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046de39-09eb-4ded-a710-cfcb29237024",
   "metadata": {},
   "source": [
    "### Evaluation Metric - NDCG\n",
    "NDCG stands for Normalized Discounted Cumulative Gain. It's a widely used metric for evaluating the performance of ranking models, especially in information retrieval and recommendation systems. NDCG is effective because it considers two key factors:\n",
    "\n",
    "1. The relevance of the recommended items.\n",
    "\n",
    "2. The position of those items in the ranked list.\n",
    "\n",
    "#### How NDCG Works\n",
    "When a recommendation system presents you with a list of articles, you are most likely to click on the ones at the very top. An ideal system would place the most relevant articles at the top and less relevant ones further down.\n",
    "\n",
    "NDCG measures this by:\n",
    "\n",
    "- Assigning a score to each item: Each recommended item is given a relevance score.\n",
    "\n",
    "- Discounting by position: The score of each item is \"discounted\" (given a lower weight) the further down it is on the list. This means a highly relevant item at the top of the list contributes much more to the final score than a highly relevant item at the bottom.\n",
    "\n",
    "- Normalizing: The final score is normalized to a value between 0 and 1 by dividing it by the score of an \"ideal\" ranking, where all the most relevant items are placed at the very top.\n",
    "\n",
    "A higher NDCG score (closer to 1) indicates that the model is doing a better job of putting the most relevant items in the highest-ranked positions, which leads to a better user experience and higher recommendation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1883c-e17b-4585-81fe-5614cdad01ee",
   "metadata": {},
   "source": [
    "### K-fold Cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b38e2-0470-4827-b1f6-c5b8965ca757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation. The 5-fold split is performed by user.\n",
    "# This section is separate from the single training and validation from before.\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_rank_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id','label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "# Perform 5-fold cross-validation and save the intermediate results for stacking.\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "\n",
    "    # Group the training and validation sets by user\n",
    "    train_idx.sort_values(by=['user_id'], inplace=True)\n",
    "    g_train = train_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "\n",
    "    valid_idx.sort_values(by=['user_id'], inplace=True)\n",
    "    g_val = valid_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "\n",
    "    # Define the model\n",
    "    lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)\n",
    "    # Train the model\n",
    "    lgb_ranker.fit(train_idx[lgb_cols], train_idx['label'], group=g_train,\n",
    "                   eval_set=[(valid_idx[lgb_cols], valid_idx['label'])], eval_group= [g_val],\n",
    "                   eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, )\n",
    "\n",
    "    # Predict validation set results\n",
    "    valid_idx['pred_score'] = lgb_ranker.predict(valid_idx[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "\n",
    "    # Normalize the output scores\n",
    "    valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n",
    "\n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "    # Append the validation set prediction results to a list for concatenation later\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "\n",
    "    # If in online mode, accumulate the test set predictions from each fold\n",
    "    if not offline:\n",
    "        sub_preds += lgb_ranker.predict(tst_user_item_feats_df_rank_model[lgb_cols], lgb_ranker.best_iteration_)\n",
    "\n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# Save the new features generated from the training set cross-validation\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path + 'trn_lgb_ranker_feats.csv', index=False)\n",
    "\n",
    "# For the test set, average the predictions from multiple folds. Save the predicted score and rank as new features for stacking.\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# Save the new features from the test set cross-validation\n",
    "tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path + 'tst_lgb_ranker_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb0af5-b4fe-4496-87ff-254ca7efafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "# Generate results for k-fold cross-validated Model.\n",
    "rank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "save_results(rank_results, topk=5, model_name='lgb_ranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11519014-88fc-4cd7-af25-4d1bff4eb9b4",
   "metadata": {},
   "source": [
    "## 5. LightGBM Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546fb80-9438-4765-aaf9-d3c4a850a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Classification Model Definition\n",
    "lgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=500, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01917af9-baeb-4ded-a7e6-28c3bc498536",
   "metadata": {},
   "source": [
    "### LightGBM Classifier Parameters\n",
    "\n",
    "* `boosting_type`: The type of boosting algorithm. We're using `'gbdt'`, which stands for Gradient Boosting Decision Tree.\n",
    "* `num_leaves`: The maximum number of leaves in each tree. The default is 31. Larger values can increase model complexity and risk overfitting.\n",
    "* `reg_alpha`: The weight for L1 regularization. The default is `0.0`, which means no L1 regularization is applied to control model complexity and sparsity.\n",
    "* `reg_lambda`: The weight for L2 regularization. The default is `1.0`, which is used to control model complexity.\n",
    "* `max_depth`: The maximum depth of each tree. The default is `-1`, which means there is no limit. A larger value can make the model more complex and prone to overfitting.\n",
    "* `n_estimators`: The number of boosting trees to train. The default is 100.\n",
    "* `subsample`: The fraction of the training data to be randomly sampled for each tree. The default is `0.7`. This helps prevent overfitting by introducing randomness.\n",
    "* `colsample_bytree`: The fraction of features (columns) to be randomly sampled for each tree. The default is `0.7`. This also helps prevent overfitting.\n",
    "* `subsample_freq`: The frequency for subsampling. The default is `1`, meaning a new random sample of data is used for every iteration.\n",
    "* `learning_rate`: The learning rate, or step size, of the gradient descent. The default is `0.01`. A smaller learning rate can make the model more stable but also slower to train.\n",
    "* `min_child_weight`: A pruning parameter that defines the minimum sum of weights of all data points required in a child leaf. The default is `50`. This helps control the model's complexity.\n",
    "* `random_state`: The random seed, set to `2018` for reproducibility.\n",
    "* `n_jobs`: The number of parallel threads to use for training. The default is `16`.\n",
    "* `verbose`: Controls the level of output during the training process. The default is `10`, which provides detailed outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f79cf-4ef8-4265-99c8-e265a391cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "if offline:\n",
    "    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'],\n",
    "                    eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])],\n",
    "                    eval_metric=['auc', ], early_stopping_rounds=50, )\n",
    "else:\n",
    "    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb897c-5c2d-4ada-9ede-ba202e8aaa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "tst_user_item_feats_df['pred_score'] = lgb_Classfication.predict_proba(tst_user_item_feats_df[lgb_cols])[:,1]\n",
    "\n",
    "# Save these ranking results for later model ensembling\n",
    "tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path + 'lgb_cls_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a31821-dd7b-4dcc-9312-304279c5113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "save_results(rank_results, topk=5, model_name='lgb_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a154f-b044-4f98-b785-b2465c98d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation. The 5-fold split is performed by user.\n",
    "# This section is separate from the single training and validation from before.\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_rank_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id', 'label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "# Perform 5-fold cross-validation and save the intermediate results for stacking.\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "\n",
    "    # Define the model and its parameters\n",
    "    lgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)\n",
    "    # Train the model\n",
    "    lgb_Classfication.fit(train_idx[lgb_cols], train_idx['label'],eval_set=[(valid_idx[lgb_cols], valid_idx['label'])],\n",
    "                          eval_metric=['auc', ], early_stopping_rounds=50, )\n",
    "\n",
    "    # Predict validation set results\n",
    "    valid_idx['pred_score'] = lgb_Classfication.predict_proba(valid_idx[lgb_cols],\n",
    "                                                              num_iteration=lgb_Classfication.best_iteration_)[:,1]\n",
    "\n",
    "    # Classification models output probabilities directly, so no normalization is needed.\n",
    "    # valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n",
    "\n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "    # Append the validation set prediction results to a list for concatenation later\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "\n",
    "    # If in online mode, accumulate the test set predictions from each fold\n",
    "    if not offline:\n",
    "        sub_preds += lgb_Classfication.predict_proba(tst_user_item_feats_df_rank_model[lgb_cols],\n",
    "                                                     num_iteration=lgb_Classfication.best_iteration_)[:,1]\n",
    "\n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# Save the new features generated from the training set cross-validation\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path + 'trn_lgb_cls_feats.csv', index=False)\n",
    "\n",
    "# For the test set, average the predictions from multiple folds. Save the predicted score and rank as new features for stacking.\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# Save the new features from the test set cross-validation\n",
    "tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path + 'tst_lgb_cls_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56671fd-7f9b-4f4b-9259-1e34dcf0b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "save_results(rank_results, topk=5, model_name='lgb_cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06b452-d90b-498a-b8b2-09848e219715",
   "metadata": {},
   "source": [
    "## 6. DIN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08169d-934d-4a97-bc2a-fe25c69d1fd8",
   "metadata": {},
   "source": [
    "DIN (Deep Interest Network) is a deep learning model for recommendation systems that's designed to better capture a user's changing interests and behavior. Unlike traditional models, it introduces the concept of \"interest evolution,\" recognizing that a user's preferences shift over time based on their past actions.\n",
    "\n",
    "#### The Core Idea: Attention Mechanism\n",
    "The central innovation of the DIN model is its use of an attention mechanism to learn how a user's interests evolve. Here's how it works:\n",
    "\n",
    "1. It takes a user's sequence of historical actions as input.\n",
    "\n",
    "2. It then looks at a candidate item (the article to be recommended) and calculates an \"attention weight\" for each of the user's historical actions.\n",
    "\n",
    "3. These attention weights show which past actions are most relevant to the user's potential interest in the current candidate. For example, if a user recently clicked on several articles about AI, the model would assign a high attention weight to those clicks when considering a new AI article.\n",
    "\n",
    "4. Finally, it combines the historical actions into a single \"interest representation\" for the user, which is then used to predict the likelihood of a click.\n",
    "\n",
    "#### Advantages of DIN\n",
    "The main strength of the DIN model is its ability to dynamically learn a user's interests. By using an attention mechanism, it can weigh historical actions differently depending on the context of the recommendation. This makes DIN particularly effective for handling sequential behavioral data and adapting to a user's evolving preferences, leading to more accurate and personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0424e-fdfd-4251-be8f-89d08769ba03",
   "metadata": {},
   "source": [
    "### User Historical Click Behavior List\n",
    "This is for the DIN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b4fda-54d0-4f4e-8cc5-d40adfa7f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    all_data = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "else:\n",
    "    trn_data = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    tst_data = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "    all_data = trn_data.append(tst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cc604-8ff9-4ae2-a292-46347dbf524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_click = all_data[['user_id', 'click_article_id']].groupby('user_id').agg({list}).reset_index()\n",
    "hist_behavior_df = pd.DataFrame()\n",
    "hist_behavior_df['user_id'] = hist_click['user_id']\n",
    "hist_behavior_df['hist_click_article_id'] = hist_click['click_article_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e303062-a880-43f1-8b5a-23b51401ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df_din_model = trn_user_item_feats_df.copy()\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_din_model = val_user_item_feats_df.copy()\n",
    "else:\n",
    "    val_user_item_feats_df_din_model = None\n",
    "\n",
    "tst_user_item_feats_df_din_model = tst_user_item_feats_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f87240-edca-44c1-8b05-df149cddbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df_din_model = trn_user_item_feats_df_din_model.merge(hist_behavior_df, on='user_id')\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_din_model = val_user_item_feats_df_din_model.merge(hist_behavior_df, on='user_id')\n",
    "else:\n",
    "    val_user_item_feats_df_din_model = None\n",
    "\n",
    "tst_user_item_feats_df_din_model = tst_user_item_feats_df_din_model.merge(hist_behavior_df, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e89e9d-1e81-41a1-b715-9bb04a03d00d",
   "metadata": {},
   "source": [
    "### An Introduction to the DIN Model\n",
    "DIN (Deep Interest Network) is a deep learning model for recommendation systems that was first introduced by Alibaba in 2018 to address the limitations of earlier models that couldn't capture the diverse interests of users. Its core innovation is an attention mechanism that dynamically represents a user's interests by considering the relevance of their historical behaviors to a specific candidate item.\n",
    "\n",
    "![](https://deepctr-doc.readthedocs.io/en/v0.8.7/_images/DIN.png)\n",
    "\n",
    "### How It Works\n",
    "Instead of creating a single, static vector to represent a user's interests, DIN uses a \"local activation unit\" that soft-searches a user's historical actions for the parts most relevant to the current candidate. This means:\n",
    "\n",
    "- High Relevance, High Weight: Past actions that are highly similar to the candidate article receive a higher activation weight, which means they have more influence on the user's final interest score.\n",
    "\n",
    "- Dynamic Interests: The user's interest vector changes depending on the candidate item. This allows the model to dynamically simulate how a user's interests shift over time, which is perfect for a task like news recommendation where timeliness is key.\n",
    "\n",
    "### Using the Deepctr Library\n",
    "We'll use the Deepctr library to implement the DIN model. Here's a brief look at the function signature:\n",
    "\n",
    "> def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False, dnn_hidden_units=(200, 80), ...)\n",
    "\n",
    "The key parameters are:\n",
    "\n",
    "* `dnn_feature_columns`: A list of all features.\n",
    "* `history_feature_list`: A list of features that represent a user's historical behavior.\n",
    "* `dnn_use_bn`: Controls whether to use Batch Normalization in the deep neural network.\n",
    "* `dnn_hidden_units`: A list or tuple that defines the number of layers and neurons in the fully connected network.\n",
    "* `dnn_activation`: The activation function for the fully connected layers (e.g., 'relu').\n",
    "* `att_hidden_size`: The number of layers and neurons in the fully connected layers of the attention network.\n",
    "* `att_activation`: The activation function for the attention layers.\n",
    "* `att_weight_normalization`: A boolean that controls whether to normalize the attention scores.\n",
    "* `l2_reg_dnn`: The L2 regularization coefficient for the fully connected layers.\n",
    "* `l2_reg_embedding`: The L2 regularization coefficient for the embedding vectors.\n",
    "* `dnn_dropout`: The dropout rate for the fully connected layers.\n",
    "* `task`: The type of task, which can be 'binary' for classification or 'regression'.\n",
    "\n",
    "### Data Preparation for DIN\n",
    "Before using the model, we need to preprocess our features into three categories:\n",
    "\n",
    "1. Sparse Features (Discrete): These are categorical features like user_id. We'll use the SparseFeat function to transform them into low-dimensional embeddings. For this, we will need to specify the column name, the number of unique values in the column, and the desired embedding dimension.\n",
    "\n",
    "2. Historical Behavior Features: These are sequences of a user's past actions, like the article_ids they've clicked on. We'll use the SparseFeat function first. A key difference in handling these features is that after we get the embedding for each item in a user's history, we need to pass them through an Attention Layer. This layer calculates how relevant each historical action is to the current candidate article. The result is a dynamic user embedding vector that reflects the user's interests based on their past clicks and the specific article being considered. This vector changes with each new candidate, allowing the model to dynamically simulate how user interests evolve over time. Therefore, we need to use VarLenSparseFeat function to pad the sequences so that they all have a uniform length (maxlen). \n",
    "\n",
    "3. Dense Features (Continuous): These are numerical features, like word counts or time differences. We'll use the DenseFeat function to define these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ba590-60b2-4a2e-854b-4e3c1390eb9c",
   "metadata": {},
   "source": [
    "### The Steps for preparing Data for the DIN Model\n",
    "1. **Initialize Feature Lists**: We start by creating three empty lists to hold our features: sparse_feature_columns for discrete (categorical) features, dense_feature_columns for continuous (numerical) features, and var_feature_columns for historical behavior sequences.\n",
    "\n",
    "2. **Define Discrete Features**: We iterate through our list of discrete feature names. For each one, we create a SparseFeat object. This object tells the model the column name, the number of unique values (plus one for unknown values), and the dimension of the embedding vector we want to use.\n",
    "\n",
    "3. **Define Continuous Features**: Similarly, we loop through our numerical feature names and create a DenseFeat object for each. Since these are single numerical values, the dimension is set to 1.\n",
    "\n",
    "4. **Define Historical Features**: For our user history, we create VarLenSparseFeat objects. These are special because they handle sequences. We first define them as a SparseFeat (giving the column name, vocabulary size, and embedding dimension) and then wrap them in VarLenSparseFeat. This second step is where we specify max_len, which is the maximum sequence length for padding and truncation.\n",
    "\n",
    "5. **Merge Feature Definitions**: We combine all three lists of feature objects (sparse, dense, and var) into a single list, dnn_feature_columns. This final list serves as the blueprint for our model's input layer.\n",
    "\n",
    "6. **Process and Prepare Data**: We create an empty dictionary x to hold the actual data. We iterate through the feature names in our blueprint list. If a feature is a historical behavior sequence, we use the pad_sequences function to make sure every sequence has a uniform length of max_len. For all other features, we simply add their values to the x dictionary.\n",
    "\n",
    "7. **Return the Data**: Finally, the function returns the prepared data dictionary x and the dnn_feature_columns list. This data is now perfectly formatted and ready to be used to train your DIN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b3340-a8d0-4c5c-98f2-6489782e4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation function\n",
    "def get_din_feats_columns(df, dense_fea, sparse_fea, behavior_fea, his_behavior_fea, emb_dim=32, max_len=100):\n",
    "    \"\"\"\n",
    "    Data preparation function.\n",
    "    df: The dataset.\n",
    "    dense_fea: A list of continuous (numerical) feature columns.\n",
    "    sparse_fea: A list of discrete (categorical) feature columns.\n",
    "    behavior_fea: A list of candidate behavior feature columns.\n",
    "    his_behavior_fea: A list of user's historical behavior feature columns.\n",
    "    embedding_dim: The dimension for the embeddings. Here, for simplicity, all discrete features use the same dimension.\n",
    "    max_len: The maximum length for user sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create SparseFeat objects for the discrete features.\n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=df[feat].nunique() + 1, embedding_dim=emb_dim) for feat in sparse_fea]\n",
    "    # Create DenseFeat objects for the continuous features.\n",
    "    dense_feature_columns = [DenseFeat(feat, 1, ) for feat in dense_fea]\n",
    "    # Create VarLenSparseFeat objects for the historical behavior features.\n",
    "    var_feature_columns = [VarLenSparseFeat(SparseFeat(feat, vocabulary_size=df['click_article_id'].nunique() + 1,\n",
    "                                    embedding_dim=emb_dim, embedding_name='click_article_id'), maxlen=max_len) for feat in hist_behavior_fea]\n",
    "    # Combine all feature objects into a single list.\n",
    "    dnn_feature_columns = sparse_feature_columns + dense_feature_columns + var_feature_columns\n",
    "\n",
    "    # Build the input dictionary 'x'.\n",
    "    x = {}\n",
    "    for name in get_feature_names(dnn_feature_columns):\n",
    "        if name in his_behavior_fea:\n",
    "            # This is a historical behavior sequence.\n",
    "            his_list = [l for l in df[name]]\n",
    "            x[name] = pad_sequences(his_list, maxlen=max_len, padding='post')  # 2D array\n",
    "        else:\n",
    "            x[name] = df[name].values\n",
    "\n",
    "    return x, dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894bf0e-67f7-40ea-be6b-5c108cfb1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features\n",
    "sparse_fea = ['user_id', 'click_article_id', 'category_id', 'click_environment', 'click_deviceGroup',\n",
    "              'click_os', 'click_country', 'click_region', 'click_referrer_type', 'is_cat_hab']\n",
    "\n",
    "behavior_fea = ['click_article_id']\n",
    "\n",
    "hist_behavior_fea = ['hist_click_article_id']\n",
    "\n",
    "dense_fea = ['sim0', 'time_diff0', 'word_diff0', 'sim_max', 'sim_min', 'sim_sum', 'sim_mean', 'score',\n",
    "             'rank','click_size','time_diff_mean','active_level','user_time_hob1','user_time_hob2',\n",
    "             'words_hbo','words_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d684884-7fb8-45c7-8083-109f04ead5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dense features. Neural network training requires that all numerical values are normalized.\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# The following is a special handling process. If there are invalid values (e.g., inf)\n",
    "# they must be handled before normalization. \n",
    "trn_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "tst_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "for feat in dense_fea:\n",
    "    trn_user_item_feats_df_din_model[feat] = mm.fit_transform(trn_user_item_feats_df_din_model[[feat]])\n",
    "\n",
    "    if val_user_item_feats_df_din_model is not None:\n",
    "        val_user_item_feats_df_din_model[feat] = mm.fit_transform(val_user_item_feats_df_din_model[[feat]])\n",
    "\n",
    "    tst_user_item_feats_df_din_model[feat] = mm.fit_transform(tst_user_item_feats_df_din_model[[feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dd87b-7b78-4afc-a783-88cb704bf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare training data\n",
    "x_trn, dnn_feature_columns = get_din_feats_columns(trn_user_item_feats_df_din_model, dense_fea,\n",
    "                                               sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "y_trn = trn_user_item_feats_df_din_model['label'].values\n",
    "\n",
    "if offline:\n",
    "    # Prepare validation data\n",
    "    x_val, dnn_feature_columns = get_din_feats_columns(val_user_item_feats_df_din_model, dense_fea,\n",
    "                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    y_val = val_user_item_feats_df_din_model['label'].values\n",
    "\n",
    "dense_fea = [x for x in dense_fea if x != 'label']\n",
    "x_tst, dnn_feature_columns = get_din_feats_columns(tst_user_item_feats_df_din_model, dense_fea,\n",
    "                                               sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6e8ba-ac16-442b-b68a-04f6b4dfeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = DIN(dnn_feature_columns, behavior_fea)\n",
    "\n",
    "# View the model structure\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e34f49-48d1-48c6-a194-489e3a9665b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if offline:\n",
    "    history = model.fit(x_trn, y_trn, verbose=1, epochs=10, validation_data=(x_val, y_val) , batch_size=256)\n",
    "else:\n",
    "    # history = model.fit(x_trn, y_trn, verbose=1, epochs=3, validation_split=0.3, batch_size=256)\n",
    "    history = model.fit(x_trn, y_trn, verbose=1, epochs=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fe179-ebfa-42c0-8952-867ec2692171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "tst_user_item_feats_df_din_model['pred_score'] = model.predict(x_tst, verbose=1, batch_size=256)\n",
    "tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path + 'din_rank_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae1de7-8d02-4b28-80d0-12000d4da657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "save_results(rank_results, topk=5, model_name='din')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f93808-1d69-4eef-bb31-214c43a106b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation. The 5-fold split here is performed by user.\n",
    "# This section is separate from the single training and validation from before.\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_din_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id', 'label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "dense_fea = [x for x in dense_fea if x != 'label']\n",
    "x_tst, dnn_feature_columns = get_din_feats_columns(tst_user_item_feats_df_din_model, dense_fea,\n",
    "                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "\n",
    "# Perform 5-fold cross-validation and save the intermediate results for stacking.\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "\n",
    "    # Prepare training data\n",
    "    x_trn, dnn_feature_columns = get_din_feats_columns(train_idx, dense_fea,\n",
    "                                                       sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    y_trn = train_idx['label'].values\n",
    "\n",
    "    # Prepare validation data\n",
    "    x_val, dnn_feature_columns = get_din_feats_columns(valid_idx, dense_fea,\n",
    "                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    y_val = valid_idx['label'].values\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_trn, y_trn, verbose=1, epochs=2, validation_data=(x_val, y_val) , batch_size=256)\n",
    "\n",
    "    # Predict validation set results\n",
    "    valid_idx['pred_score'] = model.predict(x_val, verbose=1, batch_size=256)\n",
    "\n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "    # Append the validation set prediction results to a list for concatenation later\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "\n",
    "    # If in online mode, accumulate the test set predictions from each fold\n",
    "    if not offline:\n",
    "        sub_preds += model.predict(x_tst, verbose=1, batch_size=256)[:, 0]\n",
    "\n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# Save the new features generated from the training set cross-validation\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path + 'trn_din_cls_feats.csv', index=False)\n",
    "\n",
    "# For the test set, average the predictions from multiple folds. Save the predicted score and rank as new features for stacking.\n",
    "tst_user_item_feats_df_din_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_din_model['pred_score'] = tst_user_item_feats_df_din_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_din_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_din_model['pred_rank'] = tst_user_item_feats_df_din_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# Save the new features from the test set cross-validation\n",
    "tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path + 'tst_din_cls_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3153be-16b3-44a8-a9c0-09ee476a43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "save_results(rank_results, topk=5, model_name='din')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37704-f29d-4e04-b7b9-d00b93ea463a",
   "metadata": {},
   "source": [
    "## 7. Model Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebd181-d1c5-4341-acd1-03fc37d5d8cc",
   "metadata": {},
   "source": [
    "### 7.1 Weighted Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a693fb7-a46f-4376-a5fe-9e0ba83b6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ranking results files from multiple models.\n",
    "lgb_ranker = pd.read_csv(save_path + 'lgb_ranker_score.csv')\n",
    "lgb_cls = pd.read_csv(save_path + 'lgb_cls_score.csv')\n",
    "din_ranker = pd.read_csv(save_path + 'din_rank_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7bd21-53ff-4573-9207-d4f9960429a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = {'lgb_ranker': lgb_ranker,\n",
    "              'lgb_cls': lgb_cls,\n",
    "              'din_ranker': din_ranker}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc8a74-236e-492b-b5eb-5efd1cb40a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensumble_predict_topk(rank_model, topk=5):\n",
    "    final_recall = rank_model['lgb_cls'].append(rank_model['din_ranker'])\n",
    "    rank_model['lgb_ranker']['pred_score'] = rank_model['lgb_ranker']['pred_score'].transform(lambda x: norm_sim(x))\n",
    "\n",
    "    final_recall = final_recall.append(rank_model['lgb_ranker'])\n",
    "    final_recall = final_recall.groupby(['user_id', 'click_article_id'])['pred_score'].sum().reset_index()\n",
    "\n",
    "    save_results(final_recall, topk=topk, model_name='ensemble_fuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ea454-bd45-4614-a387-023590fa0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ensumble_predict_topk(rank_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72304ffb-0ed0-4b26-a20b-694805a31903",
   "metadata": {},
   "source": [
    "### 7.2 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbe462-17df-49c6-bd93-13a9030b2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the result files generated from multiple models' cross-validation.\n",
    "# Training set\n",
    "trn_lgb_ranker_feats = pd.read_csv(save_path + 'trn_lgb_ranker_feats.csv')\n",
    "trn_lgb_cls_feats = pd.read_csv(save_path + 'trn_lgb_cls_feats.csv')\n",
    "trn_din_cls_feats = pd.read_csv(save_path + 'trn_din_cls_feats.csv')\n",
    "\n",
    "# Test set\n",
    "tst_lgb_ranker_feats = pd.read_csv(save_path + 'tst_lgb_ranker_feats.csv')\n",
    "tst_lgb_cls_feats = pd.read_csv(save_path + 'tst_lgb_cls_feats.csv')\n",
    "tst_din_cls_feats = pd.read_csv(save_path + 'tst_din_cls_feats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c87280-1740-4e0e-b251-587426bae7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the features from multiple model outputs.\n",
    "finall_trn_ranker_feats = trn_lgb_ranker_feats[['user_id', 'click_article_id', 'label']]\n",
    "finall_tst_ranker_feats = tst_lgb_ranker_feats[['user_id', 'click_article_id']]\n",
    "\n",
    "for idx, trn_model in enumerate([trn_lgb_ranker_feats, trn_lgb_cls_feats, trn_din_cls_feats]):\n",
    "    for feat in [ 'pred_score', 'pred_rank']:\n",
    "        col_name = feat + '_' + str(idx)\n",
    "        finall_trn_ranker_feats[col_name] = trn_model[feat]\n",
    "\n",
    "for idx, tst_model in enumerate([tst_lgb_ranker_feats, tst_lgb_cls_feats, tst_din_cls_feats]):\n",
    "    for feat in [ 'pred_score', 'pred_rank']:\n",
    "        col_name = feat + '_' + str(idx)\n",
    "        finall_tst_ranker_feats[col_name] = tst_model[feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb1046-51f6-4d32-af42-9203df0fdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model to fit the features generated from cross-validation and predict on the test set.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "feat_cols = ['pred_score_0', 'pred_rank_0', 'pred_score_1', 'pred_rank_1', 'pred_score_2', 'pred_rank_2']\n",
    "\n",
    "trn_x = finall_trn_ranker_feats[feat_cols]\n",
    "trn_y = finall_trn_ranker_feats['label']\n",
    "\n",
    "tst_x = finall_tst_ranker_feats[feat_cols]\n",
    "\n",
    "# Define the model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(trn_x, trn_y)\n",
    "\n",
    "# Predict the model\n",
    "finall_tst_ranker_feats['pred_score'] = lr.predict_proba(tst_x)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2958b3-6238-46e1-912c-d4e048334893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank the prediction results and save the results.\n",
    "rank_results = finall_tst_ranker_feats[['user_id', 'click_article_id', 'pred_score']]\n",
    "save_results(rank_results, topk=5, model_name='ensumble_staking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929edc2b-04fd-4d3c-96ac-85274ada2e68",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "In this section, we implemented and evaluated three distinct ranking models for a recommendation system:\n",
    "\n",
    "1. LightGBM Ranker: This is a LightGBM model specifically optimized for ranking. It learns to order items based on their relative importance, which is ideal for a recommendation task where the goal is to find the best articles to show at the top of a list.\n",
    "\n",
    "2. LightGBM Classifier: This is a LightGBM model that predicts a binary outcome—whether a user will click on an article or not. The output of this model is a probability score that can be used to rank the articles from most likely to least likely to be clicked.\n",
    "\n",
    "3. DIN (Deep Interest Network): This is a powerful deep learning model that uses an attention mechanism to dynamically capture a user's evolving interests. It's particularly effective for problems like news recommendations where a user's preferences change over time.\n",
    "\n",
    "### Model Fusion \n",
    "By implementing these three models, we have a diverse set of predictions for our test data. By combining the outputs of these different models, for example, using model fusion or a simple stacking model with logistic regression, we achieve a final result that is more accurate and robust than any single model alone. This is because each model may capture different patterns and signals from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
