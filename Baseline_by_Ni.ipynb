{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c14b4f-b28d-43c5-a5a6-d0c3da33c23c",
   "metadata": {},
   "source": [
    "# ðŸ“° News Recommendation System Part 0 - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb241b-e81b-4436-9cdc-1cf93f538193",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Project Introduction\n",
    "This project explores user behavior prediction in a news recommendation scenario. The goal is to build a model that can predict a user's future click behavior based on their historical browsing and clicking behavior data, specifically the last news article they clicked on.\n",
    "\n",
    "The setting is inspired by a real-world news app, where delivering timely, relevant content is essential for user engagement. This project aims to simulate a practical recommender system, combining business intuition with machine learning techniques to address a realistic problem in the content recommendation space.\n",
    "\n",
    "## ðŸ“Š Data Overview\n",
    "The dataset contains user interaction data from a large-scale news platform, including:\n",
    "- 300,000 users\n",
    "- ~3 million clicks\n",
    "- 360,000+ unique news articles; each news article is represented by a pre-trained embedding vector, capturing semantic relationships between articles.\n",
    "\n",
    "We extracted click log data from 200,000 users as the training set, 50,000 users as test set A, and 50,000 users as test set B.\n",
    "\n",
    "## ðŸ“„ Data Tables\n",
    "\n",
    "- train_click_log.csv: Training set user click logs\n",
    "- testA_click_log.csv: Test set user click logs\n",
    "- articles.csv: News article information data table\n",
    "- articles_emb.csv: Embedding vector representation of news articles\n",
    "\n",
    "|        **Field**        |         **Description**          |\n",
    "| :---------------------: | :------------------------------: |\n",
    "|         user_id         |              User ID             |\n",
    "|    click_article_id     |            Clicked article ID    |\n",
    "|     click_timestamp     |            Click timestamp        |\n",
    "|    click_environment    |             Click environment     |\n",
    "|    click_deviceGroup    |            Click device group     |\n",
    "|        click_os         |           Click operating system  |\n",
    "|      click_country      |             Click city            |\n",
    "|      click_region       |             Click region          |\n",
    "|   click_referrer_type   |           Click source type       |\n",
    "|       article_id        | Article ID, corresponding to click_article_id |\n",
    "|       category_id       |            Article type ID        |\n",
    "|      created_at_ts      |          Article creation timestamp |\n",
    "|       words_count       |             Article word count     |\n",
    "| emb_1,emb_2,...,emb_249 |      Article embedding vector representation |\n",
    "\n",
    "## ðŸ“ Evaluation Metrics\n",
    "The final recommendation for each user will include five recommended articles, sorted by click probability.\n",
    "\n",
    "For example, for user1, our recommendation would be:\n",
    "> user1, article1, article2, article3, article4, article5.\n",
    "\n",
    "There is only one correct answer for each user's last clicked article, so we check if any of the recommended five articles match the actual answer. We will use **mean reciprocal rank** as the evaluation metric. The formula is as follows:\n",
    "$$\n",
    "score(user) = \\sum_{k=1}^5 \\frac{s(user, k)}{k}\n",
    "$$\n",
    "\n",
    "If article1 is the actual article clicked by the user, then s(user1, 1) = 1, and s(user1, 2-4) are all 0. If article2 is the article clicked by the user, then s(user, 2) = 1/2, and s(user, 1, 3, 4, 5) are all 0. Thus, score(user) = the reciprocal of the rank at which the match occurs. If there are no matches, score(user1) = 0. This is reasonable because we want hits to be as high-ranking as possible, which yields a higher score.\n",
    "\n",
    "## ðŸ’¡ Project Understanding\n",
    "The goal of this project is to **predict the last news article a user clicked, based on their historical browsing data**. Unlike traditional structured prediction problems, this is more aligned with real-world recommendation systems, using raw user click logs rather than neatly labeled data.\n",
    "\n",
    "To approach this, I framed the task as a **supervised learning** problem by transforming user-article interactions into \"features + labels\" training data. The core idea is to predict the likelihood of a user clicking a given article, turning this into a click-through rate (CTR) prediction task. This reframing allows for the use of **classification models**â€”starting with simple baselines like logistic regression and moving toward deep learning approaches.\n",
    "\n",
    "Now, we have converted this problem into a classification problem, where the classification label is whether the user will click on a particular article. The features of the classification problem will include the user and the article. We need to train a classification model to predict the probability of a particular user clicking on a specific article. This raises several additional questions:\n",
    "- How to create training and testing datasets?\n",
    "- What specific features can we leverage?\n",
    "- What models can we attempt?\n",
    "- With 360,000 articles and over 200,000 users, what strategies do we have to reduce the problem's scale? How do we make the final predictions?\n",
    "\n",
    "**For this beginning part, we will run a baseline for our news recommendation system project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38326fb-be22-4b74-a7ef-f734dfc99f05",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20ac06-0616-4656-a986-9f7376722392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time  # Import the time module for handling time-related operations.\n",
    "import math  # Import the math module, which provides mathematical functions and constants.\n",
    "import os  # Import the os module for interacting with the operating system.\n",
    "from tqdm import tqdm  # Import the tqdm module for creating progress bars to visualize the progress of iterations.\n",
    "import gc  # Import the garbage collection module for releasing memory space.\n",
    "import pickle  # Import the pickle module for serializing and deserializing Python objects.\n",
    "import random  # Import the random module for generating random numbers.\n",
    "from datetime import datetime  # Import the datetime module for handling date and time.\n",
    "from operator import itemgetter  # Import the itemgetter function from the operator module for retrieving elements based on an index or key.\n",
    "import numpy as np  # Import the NumPy library, which provides high-performance numerical computing capabilities.\n",
    "import pandas as pd  # Import the Pandas library, which offers data analysis and manipulation functionalities.\n",
    "import warnings  # Import the warnings module for controlling the display of warning messages.\n",
    "from collections import defaultdict  # Import the defaultdict class from the collections module, which provides a dictionary that allows setting default values.\n",
    "import collections  # Import the collections module, which provides commonly used collection classes.\n",
    "warnings.filterwarnings('ignore')  # Ignore the display of warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ee096-0e5a-4875-982c-7b9616c96ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, use this cell to load data\n",
    "from google.colab import drive\n",
    "\n",
    "# Connect to Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define file paths\n",
    "data_path = '/content/drive/MyDrive/Datasets/news-rec-sys/'\n",
    "save_path = '/content/drive/MyDrive/Datasets/news-rec-sys/temp_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590ca44-e599-49a1-ba18-4fb042abaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a local machine\n",
    "data_path = './data/' \n",
    "save_path = './data/temp_results/' # save temperary result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa53cb0-d278-44ae-8160-1eb4c110c4e1",
   "metadata": {},
   "source": [
    "### df Save storage by using sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e38018-eb7b-4903-b0f2-2e84c4d6f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard function for memory optimization\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()  # Record the start time of the function\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  # List of numeric data types\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Calculate the memory usage of the DataFrame (in Mb)\n",
    "\n",
    "    # Iterate through each column of the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes  # Get the data type of the column\n",
    "        if col_type in numerics:  # If the column's data type is numeric\n",
    "            c_min = df[col].min()  # Get the minimum value in the column\n",
    "            c_max = df[col].max()  # Get the maximum value in the column\n",
    "\n",
    "            # Check if there are missing values in the minimum and maximum values\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "\n",
    "            # Choose the appropriate data type conversion based on the data type's range\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # Calculate the memory usage of the DataFrame after conversion (in Mb)\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction), time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                  100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                  (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ce387-a6f2-4c22-a422-92bbf16b09d5",
   "metadata": {},
   "source": [
    "## 2. Read Sampled or Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251caee-29e3-4205-b715-26004760597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode: Sample a portion of data from the training set for code debugging\n",
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    \"\"\"\n",
    "    Samples a portion of data from the training set for debugging purposes.\n",
    "    data_path: The path where the original data is stored.\n",
    "    sample_nums: The number of samples to take (can be a small number of users due to memory limitations).\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')  \n",
    "    all_user_ids = all_click.user_id.unique()  # Get unique identifiers for all users\n",
    "\n",
    "   # Randomly select a specified number of users from all users as sampled users\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False)\n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]  # Retain click data for sampled users\n",
    "\n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # åŽ»é™¤é‡å¤çš„ç‚¹å‡»æ•°æ®\n",
    "    return all_click\n",
    "\n",
    "# Read click data, which is divided into online and offline. If it is for online submission results, the test set's click data should be merged into the overall data.\n",
    "# If it is for offline validation of the model's effectiveness or feature effectiveness, you can use only the training set.\n",
    "def get_all_click_df(data_path='./data_raw/', offline=True):\n",
    "    if offline:\n",
    "        all_click = pd.read_csv(data_path + 'train_click_log.csv')  # Read the click data from the training set\n",
    "    else:\n",
    "        trn_click = pd.read_csv(data_path + 'train_click_log.csv') # Read the click data from the training set\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')   # Read the click data from the test set\n",
    "\n",
    "        all_click = trn_click.append(tst_click)  # Combine the click data from the training set and test set\n",
    "\n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # Remove duplicate click data\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e3cca-07f9-410d-9b00-9b40199ea998",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df = get_all_click_df(data_path, offline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f6107-bc52-4289-8b2c-92b1a20cc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a049c-93ab-432e-ac95-58fc8b6acd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3712a-2994-42f6-9904-1cc65f489ac0",
   "metadata": {},
   "source": [
    "## 3. Create a dictionary that maps users to articles and their corresponding click times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31397-5971-4c7e-aac0-2a5822032da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user's article-click time sequence based on click time: {user1: [(item1, time1), (item2, time2), ...]...}\n",
    "def get_user_item_time(click_df):\n",
    "    # Sort the click DataFrame by click timestamp\n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "\n",
    "    def make_item_time_pair(df):\n",
    "        # Create a list of article IDs and corresponding timestamps\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "\n",
    "    # Group by user, generate a list of article ID and timestamp pairs for each user, reset the index, and rename the columns\n",
    "    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']].apply(lambda x: make_item_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    # Create a dictionary with user IDs as keys and lists of article ID and timestamp pairs as values\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "\n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c55de-6a21-4a33-9b72-1fc2fe308d36",
   "metadata": {},
   "source": [
    "## 4. Retrieve the top k clicked article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7280e-9dcb-4089-9ea0-ef0a7b0ef80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recently clicked articles\n",
    "def get_item_topk_click(click_df, k):\n",
    "    \"\"\"\n",
    "    Get the top k article IDs with the most clicks.\n",
    "\n",
    "    Parameters:\n",
    "        click_df: DataFrame containing click data.\n",
    "        k: Number of top article IDs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        topk_click: List of the top k article IDs with the most clicks.\n",
    "    \"\"\"\n",
    "    # Use the value_counts() function to count the occurrences of each article ID in the click_df DataFrame's click_article_id column.\n",
    "    # Then, use index slicing [:k] to select the top k articles with the highest click counts.\n",
    "    topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "    return topk_click"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347e49e-76e0-436b-a544-8944f78b4bab",
   "metadata": {},
   "source": [
    "## 5. itemcf Calculation of the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9f21e-4776-4324-87b9-2b66a402b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df):\n",
    "    \"\"\"\n",
    "    Calculation of the similarity matrix between articles\n",
    "\n",
    "    Parameters:\n",
    "        df: Data table\n",
    "        item_created_time_dict: Dictionary of article creation times\n",
    "\n",
    "    Returns:\n",
    "        i2i_sim_: Matrix of similarity between articles\n",
    "\n",
    "    Idea:\n",
    "    Collaborative filtering based on items (for details, refer to the previous team learning on basic recommendation systems).\n",
    "    In the multi-recall section, a recall strategy based on association rules will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call a function to obtain a dictionary of user-item-click time data\n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "\n",
    "    # Calculate item similarity\n",
    "    i2i_sim = {}  # Store a dictionary of item-item similarity\n",
    "    item_cnt = defaultdict(int)  # Dictionary to count item occurrences\n",
    "\n",
    "    # Iterate through the user-item-click time data dictionary\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        # Consider time factors when optimizing item-based collaborative filtering\n",
    "\n",
    "        # Iterate through the list of items and their click times for the same user\n",
    "        for i, i_click_time in item_time_list:\n",
    "            # Count item occurrences\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "\n",
    "            # Iterate through the list of other items and their click times for the same user\n",
    "            for j, j_click_time in item_time_list:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                i2i_sim[i][j] += 1 / math.log(len(item_time_list) + 1)\n",
    "\n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "\n",
    "    # Further process and optimize the similarity dictionary\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "\n",
    "    # Save the obtained similarity matrix locally\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "\n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b11b92-d20f-4d50-881d-9453689d0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b020cdd-1d3d-4496-876e-d414e26ff0f5",
   "metadata": {},
   "source": [
    "## 6. itemcf article recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87e755-7f20-425d-86fc-7bcb4a65d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click):\n",
    "    \"\"\"\n",
    "    Recommendation based on item-based collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "        user_id: User ID\n",
    "        user_item_time_dict: Dictionary of user-clicked article sequences based on click time {user1: [(item1, time1), (item2, time2), ...]...}\n",
    "        i2i_sim: Dictionary, article similarity matrix\n",
    "        sim_item_topk: Integer, choose the top k most similar articles to the current article\n",
    "        recall_item_num: Integer, the number of recalled articles in the end\n",
    "        item_topk_click: List, the list of most-clicked articles for user recall completion\n",
    "    \n",
    "    Returns:\n",
    "        item_rank: Recommended articles {item1: score1, item2: score2, ...}\n",
    "        \n",
    "    Note: In the multi-recall part, a recall strategy based on association rules will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the articles that the user has interacted with in the past\n",
    "    user_hist_items = user_item_time_dict[user_id]  # Get the list of articles the user has clicked on\n",
    "    user_hist_items_ = {user_id for user_id, _ in user_hist_items}  # Convert the list of articles the user has clicked on to a set for easy lookup\n",
    "\n",
    "    item_rank = {}  # Store a dictionary of articles and their similarity scores\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        # Iterate through the list of articles the user has clicked on and their corresponding click times\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "            # Iterate through the top sim_item_topk articles most similar to the current article and their similarity scores\n",
    "            if j in user_hist_items_:\n",
    "                continue  # If the similar article is already in the user's historical click list, skip it\n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += wij  # Accumulate the similarity score of the similar articles\n",
    "\n",
    "    # If there are less than recall_item_num articles, complete with popular items\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items():  # If the completed article is already in the previous list, skip it\n",
    "                continue\n",
    "            item_rank[item] = -i - 100  # Assign a negative score to the completed article (arbitrarily set)\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break  # Exit the loop after reaching the specified number of recalled articles\n",
    "\n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]  # Sort articles in descending order based on score and truncate to the specified number of articles\n",
    "\n",
    "    return item_rank  # Return the list of recalled articles, including articles and their scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9ce25-28cf-451b-908e-eeadae7bb2c3",
   "metadata": {},
   "source": [
    "## 7. Recommend articles for each user based on item-based collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b3a22-b088-4a61-b480-468f65f82ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Get the user-item-click time dictionary\n",
    "user_item_time_dict = get_user_item_time(all_click_df)\n",
    "\n",
    "# Load item-item similarity\n",
    "i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "# Number of similar articles to consider\n",
    "sim_item_topk = 10\n",
    "\n",
    "# Number of recalled articles\n",
    "recall_item_num = 10\n",
    "\n",
    "# User recall completion with popular items\n",
    "item_topk_click = get_item_topk_click(all_click_df, k=50)\n",
    "\n",
    "# Loop through all users\n",
    "for user in tqdm(all_click_df['user_id'].unique()):\n",
    "    # Recommend articles for each user based on item-based collaborative filtering\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim,\n",
    "                                                        sim_item_topk, recall_item_num, item_topk_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608e023-a007-4b29-8103-d63c5bbe8ddd",
   "metadata": {},
   "source": [
    "## 8. Transform recall dictionary to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fd4f1-4f2a-4165-a617-0bbb90ec91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_score_list = []\n",
    "\n",
    "for user, items in tqdm(user_recall_items_dict.items()):\n",
    "    for item, score in items:\n",
    "        user_item_score_list.append([user, item, score])\n",
    "\n",
    "recall_df = pd.DataFrame(user_item_score_list, columns=['user_id', 'click_article_id', 'pred_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab8ed9-85e2-4ddb-9c50-ef5c587e389c",
   "metadata": {},
   "source": [
    "## 9. Save Recommendation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8157b-295c-4642-8ebc-777cea561b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submit file\n",
    "def submit(recall_df, topk=5, model_name=None):\n",
    "    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n",
    "    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "    # detect wether every user has 5 articles\n",
    "    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n",
    "    assert tmp.min() >= topk\n",
    "\n",
    "    del recall_df['pred_score']\n",
    "    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n",
    "\n",
    "    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n",
    "    # define column name\n",
    "    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2',\n",
    "                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n",
    "\n",
    "    save_name = save_path + model_name + '_' + datetime.today().strftime('%m-%d') + '.csv'\n",
    "    submit.to_csv(save_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b1258-360b-4792-b557-aff32204680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set\n",
    "tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "tst_users = tst_click['user_id'].unique()\n",
    "\n",
    "# select the users from the recall data that are in the test set, you can follow these steps\n",
    "tst_recall = recall_df[recall_df['user_id'].isin(tst_users)]\n",
    "\n",
    "# submit final file, which is the baseline\n",
    "submit(tst_recall, topk=5, model_name='itemcf_baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
